<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:image" content="https://priml2021.github.io/img/preview-image.png"/>
    <meta property="og:image:alt" content="Privacy in Machine Learning (PriML) NeurIPS 2021 Workshop"/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:title" content="Privacy in Machine Learning (NeurIPS 2021 Workshop)"/>
    <meta name="twitter:image" content="https://priml-workshop.github.io/img/twitter-img.png"/>
    <meta name="twitter:image:alt" content="PriML Workshop"/>
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Privacy in Machine Learning (NeurIPS 2021 Workshop)</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <link href="css/style.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
    <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

<!-- Navigation -->
<nav class="navbar navbar-custom navbar-fixed-top">
    <div class="container">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                Menu <i class="fa fa-bars"></i>
            </button>
            <a class="navbar-brand page-scroll" href="#page-top">
                <span class="light">P<span style="text-transform:lowercase">ri</span>ML'21</span>
            </a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
            <ul class="nav navbar-nav">
                <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                <li class="hidden">
                    <a href="#page-top"></a>
                </li>
                <li>
                    <a class="page-scroll" href="#about">Scope</a>
                </li>
                <li>
                    <a class="page-scroll" href="#dates">CFP &amp Dates</a>
                </li>
                <li>
                    <a class="page-scroll" href="#speakers">Invited Speakers</a>
                </li>
                <li>
                    <a class="page-scroll" href="#schedule">Schedule</a>
                </li>
                <li>
                    <a class="page-scroll" href="#papers">Accepted Papers</a>
                </li>
                <!-- <li>
                    <a class="page-scroll" href="#grants">Travel Grants</a>
                </li> -->
                <li>
                    <a class="page-scroll" href="#organizers">Organizers</a>
                </li>
                <li>
                    <a class="page-scroll" href="#access">Accessibility</a>
                </li>
            </ul>
        </div>
    </div>
</nav>

<!-- Intro Header -->
<header class="intro">
    <div class="intro-body">
        <div class="container">
            <div class="row">
                <div class="col-md-8 col-md-offset-2">
                    <h1 class="brand-heading">Privacy in Machine Learning</h1>
                    <p class="intro-text">
                        <a href="https://neurips.cc/" style="color:white">NeurIPS 2021</a> Workshop
                        <br/>Virtual
                    </p>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- About Section -->
<section id="about" class="container content-section text-center">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2">
            <h2>Scope</h2>
            <p> This one-day workshop focuses on privacy-preserving machine learning techniques for large-scale data
                analysis, both in the distributed and centralized settings, and on scenarios that highlight the
                importance and need for these techniques (e.g., via privacy attacks). There is growing interest from the
                Machine Learning (ML) community in leveraging cryptographic techniques such as Multi-Party Computation
                (MPC) and Homomorphic Encryption (HE) for secure computation during training and inference, as well as
                Differential Privacy (DP) for limiting the privacy risks from the trained model itself. We encourage
                both theory and application-oriented submissions exploring a range of approaches listed below.</p>
            <ul class="list-group">
                <li class="list-group-item speaker">Privacy-preserving machine learning</li>
                <li class="list-group-item speaker">Differential privacy: theory, applications, and implementations
                </li>
                <li class="list-group-item speaker">Statistical and information-theoretic notions of privacy, including
                    DP relaxations
                </li>
                <li class="list-group-item speaker">Empirical and theoretical comparisons between different notions
                    of privacy
                </li>
                <li class="list-group-item speaker">Privacy-preserving data sharing, anonymization, and privacy of
                    synthetic data
                </li>
                <li class="list-group-item speaker">Privacy attacks</li>
                <li class="list-group-item speaker">Federated and decentralized privacy-preserving algorithms</li>
                <li class="list-group-item speaker">Policy-making aspects of data privacy</li>
                <li class="list-group-item speaker">Secure multi-party computation techniques for machine learning</li>
                <li class="list-group-item speaker">Learning on encrypted data, homomorphic encryption</li>
                <li class="list-group-item speaker">Privacy in autonomous systems</li>
                <li class="list-group-item speaker">Online social networks privacy</li>
                <li class="list-group-item speaker">Privacy and private learning in computer vision and natural language
                    processing tasks
                </li>
                <li class="list-group-item speaker">Programming languages for privacy-preserving data analysis</li>
                <li class="list-group-item speaker">Relations of privacy with fairness, transparency and adversarial
                    robustness
                </li>
                <li class="list-group-item speaker">Machine unlearning and data-deletion</li>
            </ul>
        </div>
    </div>
</section>

<!-- CFP & Dates Section -->
<section id="dates" class="container content-section text-center">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2">
            <h2>Call For Papers &amp; Important Dates</h2>
            <a href="cfp-priml21.txt" class="btn btn-default btn-lg">Download Full CFP</a>
            <br/>
            <br/>
            <br/>
            <p>
                <b>Submission deadline</b>: September <S> 16 </S> 17, 2021 (UTC)
                <br/><b>Notification of acceptance</b>: October <S> 15 </S> 19, 2021
                <br/><b>Video and slides submission deadline (for accepted papers)</b>: November 1, 2021
                <br/><b>Event date</b>: December 14, 2021
                <br/><b>Contact </b>: privacymlworkshop@gmail.com
            </p>
            <h3>Submission Instructions</h3>
            <p>

                Submissions in the form of extended abstracts must be at most 4 pages long (not including references;
                additional supplementary material may be submitted but may be ignored by reviewers), non-anonymized, and
                adhere to the NeurIPS format. We encourage the submission of work that is new to the privacy-preserving
                machine learning community. Submissions solely based on work that has been previously published in
                conferences on machine learning and related fields are not suitable for the workshop. On the other hand,
                we allow submission of works currently under submission and relevant works recently previously published
                in privacy and security venues. Submission of work under review at NeurIPS 2021 is allowed but this must
                be disclosed at submission time. Submissions accepted to the NeurIPS main conference may be
                deprioritized in selecting oral presentations. The workshop will not have formal proceedings, but
                authors of accepted abstracts can choose to have a link to arxiv or a pdf added on the workshop webpage.

            </p>
            <p><b><a href="https://openreview.net/group?id=NeurIPS.cc/2021/Workshop/PRIML">Submit Your Abstract Here</a>
                <!-- <a href="https://easychair.org/conferences/?conf=priml2019" class="btn btn-default btn-lg">Submit Your Abstract</a> -->
                <br/>
                <br/>
                <br/>
        </div>
    </div>
</section>

<!-- Speakers Section -->
<section id="speakers" class="container content-section text-center">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2">
            <h2>Invited Speakers</h2>
            <ul class="list-group">
                <li class="list-group-item speaker">Helen Nissenbaum (Cornell Tech)</li>
                <li class="list-group-item speaker">Emiliano de Cristofaro (University College London)</li>
                <li class="list-group-item speaker">Kristin Lauter (Facebook AI Research)</li>
                <li class="list-group-item speaker">Aaron Roth (UPenn / Amazon)</li>
            </ul>
        </div>
    </div>
</section>

<section id="schedule" class="container content-section text-center">
    <div class="row">
        <div class="col-sm-8 col-sm-offset-2">
            <h2>Schedule</h2>
            <table class="table schedule">
                <tbody>
                <tr>
                     <td colspan="2"  >Block A Paris time</td>
                </tr>
                <tr>
                    <td class="time">10:20-10:30</td>
                    <td class="slot">Welcome</td>
                </tr>
                <tr>
                    <td class="time">10:30-11:00</td>
                    <td class="slot talk">
                        Invited talk:
                        <a href="#tabs1" data-toggle="collapse" class="accordion-toggle">
                            Emiliano de Cristofaro (University College London)
                        </a>
                    </td>
                <tr>
                    <td colspan="2" class="hiddenRow">
                        <div class="accordion-body collapse talk-abstract" id="tabs1">
                            Privacy in Machine Learning -- It's Complicated
                        </div>
                    </td>
                </tr>
                <tr>
                    <td class="time">11:00-11:15</td>
                    <td class="slot talk">
                        Emiliano de Cristofaro Q & A:
                        <a href="#tabs2" data-toggle="collapse" class="accordion-toggle">
                        </a>
                    </td>
                <tr>
                    <td colspan="2" class="hiddenRow">
                        <div class="accordion-body collapse talk-abstract" id="tabs2">
                        </div>
                    </td>
                </tr>
                </tr>
                <tr>
                    <td class="time">11:15-11:30</td>
                    <td class="slot">Break</td>
                </tr>
                <tr>
                    <td class="time">11:30-11:45</td>
                    <td class="slot talk">
                        <a href="#tabs3" data-toggle="collapse" class="accordion-toggle">
                            Differential Privacy via Group Shuffling
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">
                                     Amir Mohammad Abouei, Clement Louis Canonne
                                 </span>
                    </td>
                </tr>
                <tr>
                    <td colspan="2" class="hiddenRow">
                        <div class="accordion-body collapse talk-abstract" id="tabs3">
                            The past decade has seen data privacy emerge as a fundamental and pressing issue. Among the
                            tools developed to tackle it, differential privacy has emerged as a central and principled
                            framework, with specific variants capturing various threat models. In particular, the recently
                            proposed shuffle model of differential privacy allows for promising tradeoffs between accuracy
                            and privacy. However, the shuffle model may not be suitable in all situations, as it relies on
                            a distributed setting where all users can coordinate and trust (or simulate) a joint shuffling algorithm.
                            To address this, we introduce a new model, the group shuffle model, in which users are partitioned
                            into several groups, each group having its own local shuffler. We investigate the privacy/accuracy
                            tradeoffs in our model, by comparing it to both the shuffle and local models of privacy, which
                            it some sense interpolates between. In addition to general relations between group shuffle,
                            shuffle, and local privacy, we provide a detailed comparison of the cost and benefit of the
                            group shuffle model, by providing both upper and lower bounds for the specific task of binary summation.<br/>

                        </div>
                    </td>
                </tr>
                <tr>
                    <td class="time">11:45-12:00</td>
                    <td class="slot talk"><a href="#tabs4" data-toggle="collapse" class="accordion-toggle">
                        SoK: Privacy-preserving Clustering (Extended Abstract)
                    </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">
                                   Aditya Hegde, Helen Möllering, Thomas Schneider, Hossein Yalame
                                 </span>
                    </td>
                </tr>
                <tr>
                    <td colspan="2" class="hiddenRow">
                        <div class="accordion-body collapse talk-abstract" id="tabs4">
                            Clustering is a popular unsupervised machine learning technique that groups similar input
                            elements into clusters. In many applications, sensitive information is clustered that should
                            not be leaked. Moreover, nowadays it is often required to combine data from multiple sources
                            to increase the quality of the analysis as well as to outsource complex computation to powerful
                            cloud servers. This calls for efficient privacy-preserving clustering. In this work, we
                            systematically analyze the state-of-the-art in privacy-preserving clustering. We implement
                            and benchmark today's four most efficient fully private clustering protocols by Cheon et al.
                            (SAC'19), Meng et al. (ArXiv'19), Mohassel et al. (PETS'20), and Bozdemir et al. (ASIACCS'21)
                            with respect to communication, computation, and clustering quality.
                        </div>
                    </td>
                </tr>
                <tr>
                	<td class="time">12:00-12：15</td>
                    <td class="slot">Contribute talk Q&A</td>
                </tr>
                <tr>
                	<td class="time">12:15-12：30</td>
                    <td class="slot">Coffee Break</td>
                </tr>
				<tr> 
                    <td class="time">12:30-13:30</td>
                    <td class="slot">Poster</td>
                </tr>
                <tr>
                    <td class="time">13:30:14:15</td>
                    <td class="slot talk"><a href="#tabs5" data-toggle="collapse" class="accordion-toggle">
                        Panel
                    </a>
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">

                                 </span>
                    </td>
                </tr>
				<tr>
                     <td colspan="2"  >Block B LA time</td>
                </tr>
				<tr>
                    <td class="time">8:20-8:30</td>
                    <td class="slot">Welcome</td>
                </tr>
                <tr>
                    <td class="time">8:30-9:00</td>
                    <td class="slot talk">
                        Invited talk:
                        <a href="#tabs6" data-toggle="collapse" class="accordion-toggle">
                            Helen Nissenbaum (Cornell Tech)
                        </a>
                    </td>
                <tr>
                    <td colspan="2" class="hiddenRow">
                        <div class="accordion-body collapse talk-abstract" id="tabs6">
                            Practical Privacy, Fairness, Ethics, Policy
                        </div>
                    </td>
                </tr>
				<tr>
                    <td class="time">9:00-9:30</td>
                    <td class="slot talk">
                        Invited talk:
                        <a href="#tabs7" data-toggle="collapse" class="accordion-toggle">
                           Aaron Roth (UPenn / Amazon)
                        </a>
                    </td>
                <tr>
                    <td colspan="2" class="hiddenRow">
                        <div class="accordion-body collapse talk-abstract" id="tabs7">
                            Machine Unlearning.
                        </div>
                    </td>
                </tr>
                <tr>
                <tr>
                    <td class="time">9:30-10:00</td>
                    <td class="slot talk">
                         Q & A for Helen and Aaron:
                        <a href="#tabs8" data-toggle="collapse" class="accordion-toggle">
                        </a>
                    </td>
                </tr>
				<tr>
                    <td class="time">10:00-10:15</td>
                    <td class="slot">Break</td>
                </tr>
				<tr>
                    <td class="time">10:15-11:15</td>
                    <td class="slot">Poster Session</td>
                </tr>
				<tr>
                    <td class="time">11:15-11:30</td>
                    <td class="slot">Break</td>
                </tr>
				<tr>
                    <td class="time">11:30-12:00</td>
                    <td class="slot talk">
                        Invited talk:
                        <a href="#tabs15" data-toggle="collapse" class="accordion-toggle">
                            Kristin Lauter (Facebook AI Research):
                        </a>
                    </td>
                <tr>
                    <td colspan="2" class="hiddenRow">
                        <div class="accordion-body collapse talk-abstract" id="tabs15">
                            ML on Encrypted Data.
                        </div>
                    </td>
                </tr>
                <tr>
                 <tr>
                    <td class="time">12:00-12:15</td>
                    <td class="slot talk">
                         Kristin Lauter Q & A :
                        <a href="#tabs1" data-toggle="collapse" class="accordion-toggle">
                        </a>
                    </td>
                </tr>
				<tr>
                    <td class="time">12:15-12:30</td>
                    <td class="slot talk">
                        <a href="#tabs8" data-toggle="collapse" class="accordion-toggle">
                            Privacy-Aware Rejection Sampling
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">
                                     Jordan Awan, Vinayak Rao
                                 </span>
                    </td>
                </tr>
                <tr>
                    <td colspan="2" class="hiddenRow">
                        <div class="accordion-body collapse talk-abstract" id="tabs8">
                            Differential privacy (DP) offers strong protection against adversaries with arbitrary side-information
                            and computational power. However, many implementations of DP mechanisms leave themselves vulnerable
                            to side channel attacks, such as timing attacks. As many privacy mechanisms, such as the exponential
                            mechanism, do not lend themselves to easy implementations, when sampling methods such as MCMC or
                            rejection sampling are used, the runtime can leak privacy. In this work, we quantify the privacy
                            cost due to the runtime of a rejection sampler in terms of  -DP. We also propose three modifications
                            to the rejection sampling algorithm, to protect against timing attacks by making the runtime
                            independent of the data. We also use our techniques to develop an adaptive rejection sampler
                            for log-Holder densities, which also has data-independent runtime.
                        </div>
                    </td>
                </tr>
                <tr>
                    <td class="time">12:30-12:45</td>
                    <td class="slot talk">
                        <a href="#tabs9" data-toggle="collapse" class="accordion-toggle">
                            Population Level Privacy Leakage in Binary Classification wtih Label Noise
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">
                                     Robert Istvan Busa-Fekete, Andres Munoz medina, Umar Syed, Sergei Vassilvitskii
                                 </span>
                    </td>
                </tr>
                <tr>
                    <td colspan="2" class="hiddenRow">
                        <div class="accordion-body collapse talk-abstract" id="tabs9">
                            We study the privacy limitations of label differential privacy. Label differential privacy has
                            emerged as an intermediate trust model between local and central differential privacy, where
                            only the label of each training example is protected (and the features are assumed to be public).
                            We show that the guarantees provided by label DP are significantly weaker than they appear,
                            as an adversary can "un-noise" the perturbed labels. Formally we show that the privacy loss
                            has a close connection with Jeffreys' divergence of the conditional distribution between
                            positive and negative labels, which allows explicit formulation of the trade-off between
                            utility and privacy in this setting. Our results suggest how to select public features that
                            optimize this trade-off. But we still show that there is no free lunch --- instances where
                            label differential privacy guarantees are strong are exactly those where a good classifier
                            does not exist. We complement the negative results with a non-parametric estimator for the
                            true privacy loss, and apply our techniques on large-scale benchmark data to demonstrate how
                            to achieve a desired privacy protection.
                        </div>
                    </td>
                </tr>
                <tr>
                    <td class="time">12:45-13:00</td>
                    <td class="slot talk">
                        <a href="#tabs10" data-toggle="collapse" class="accordion-toggle">
                           Simple Baselines Are Strong Performers for Differentially Private Natural Language Processing
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">
                                    Xuechen Li, Florian Tramer, Percy Liang, Tatsunori Hashimoto
                                 </span>
                    </td>
                </tr>
                <tr>
                    <td colspan="2" class="hiddenRow">
                        <div class="accordion-body collapse talk-abstract" id="tabs10">
                            Differentially private learning has seen limited success for deep learning models of text,
                            resulting in a perception that differential privacy may be incompatible with the language
                            model fine-tuning paradigm. We demonstrate that this perception is inaccurate and that with
                            the right setup, high performing private models can be learned on moderately-sized corpora by
                            directly fine-tuning with differentially private optimization. Our work highlights the important
                            role of hyperparameters, task formulations, and pretrained models. Our analyses also show that
                            the low performance of naive differentially private baselines in prior work is attributable
                            to suboptimal choices in these factors. Empirical results reveal that differentially private
                            optimization does not suffer from dimension-dependent performance degradation with pretrained
                            models and achieves performance on-par with state-of-the-art private training procedures and
                            strong non-private baselines.
                        </div>
                    </td>
                </tr>
                <tr>
                    <td class="time">13:00-13:15</td>
                    <td class="slot talk">
                        <a href="#tabs11" data-toggle="collapse" class="accordion-toggle">
                           Canonical Noise Distributions and Private Hypothesis Tests
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">
                                  Jordan Awan, Salil Vadhan
                                 </span>
                    </td>
                </tr>
                <tr>
                    <td colspan="2" class="hiddenRow">
                        <div class="accordion-body collapse talk-abstract" id="tabs11">
                            In the setting of -DP, we propose the concept \emph{canonical noise distribution} (CND) which
                            captures whether an additive privacy mechanism is tailored for a given , and give a construction
                            of a CND for an arbitrary tradeoff function . We show that private hypothesis tests are intimately
                            related to CNDs, allowing for the release of private -values at no additional privacy cost as
                            well as the construction of uniformly most powerful (UMP) tests for binary data. We apply our
                            techniques to difference of proportions testing.
                        </div>
                    </td>
                </tr>
                <tr>
                	<td class="time">13:15-13：45</td>
                    <td class="slot">Q&A for four contributed talks</td>
                </tr>
                <tr>
                    <td class="time">13:45-14:30</td>
                    <td class="slot talk"><a href="#tabs5" data-toggle="collapse" class="accordion-toggle">
                        Panel
                    </a>
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">

                                 </span>
                    </td>
                </tr>
                <tr>
                	<td class="time">14:30-14：40</td>
                    <td class="slot">Closing</td>
                </tr>
                </tbody>
            </table>
        </div>
    </div>
</section>

<!-- Accepted Papers -->

<section id="papers" class="container content-section text-center">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2">
            <h2>Accepted Papers</h2>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Antti Koskela, Mikko A. Heikkilä, Antti Honkela
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs1" class="paper-title">
                        Tight Accounting in the Shuffle Model of Differential Privacy
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=ZO6uneMKak0" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot A2]</a>
                </div>
                <div id="abs1" class="panel-footer panel-paper-footer collapse">
                    Shuffle model of differential privacy is a novel distributed privacy model based on a combination of
                    local privacy mechanisms and a trusted shuffler. It has been shown that the additional randomisation
                    provided by the shuffler improves privacy bounds compared to the purely local mechanisms. Accounting
                    tight bounds, especially for multi-message protocols, is complicated by the complexity brought
                    by the shuffler. The recently proposed Fourier Accountant for evaluating -differential privacy
                    guarantees has been shown to give tighter bounds than commonly used methods for non-adaptive
                    compositions of various complex mechanisms. In this paper we show how to compute tight privacy
                    bounds using the Fourier Accountant for multi-message versions of several ubiquitous mechanisms
                    in the shuffle model and demonstrate looseness of the existing bounds in the literature.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                         Akash Bharadwaj, Graham Cormode
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs2" class="paper-title">
                        Sample-and-threshold differential privacy: Histograms and applications
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=msTLiku_34p" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot A3]</a>
                </div>
                <div id="abs2" class="panel-footer panel-paper-footer collapse">
                    Federated analytics aims to compute accurate statistics from distributed datasets.
                    A "Differential Privacy" (DP) guarantee is usually desired by the users of the devices storing the
                    data.
                    In this work, we prove a strong (&epsilon;, &delta;)-DP guarantee for a highly practical
                    sampling-based procedure to derive
                    histograms. We also provide accuracy guarantees and show how to apply the procedure to estimate
                    quantiles and modes.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Cangxiong Chen, Neill D. F. Campbell
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs3" class="paper-title">
                        Understanding Training-Data Leakage from Gradients in Neural Networks for ImageClassifications
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=c63rqIcw66h" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot B0]</a>
                </div>
                <div id="abs3" class="panel-footer panel-paper-footer collapse">
                    Federated learning of deep learning models for supervised tasks, e.g. image classification and
                    segmentation, has found many applications: for example in human-in-the-loop tasks such as film
                    post-production where it enables sharing of domain expertise of human artists in an efficient
                    and effective fashion. In many such applications, we need to protect the training data from being
                    leaked when gradients are shared in the training process due to IP or privacy concerns. Recent
                    works have demonstrated that it is possible to reconstruct the training data from gradients for
                    an image-classification model when its architecture is known. However, there is still an incomplete
                    theoretical understanding of the efficacy and failure of such attacks. In this paper, we analyse
                    the source of training-data leakage from gradients. We formulate the problem of training data
                    reconstruction as solving an optimisation problem iteratively for each layer. The layer-wise
                    objective function is primarily defined by weights and gradients from the current layer as well
                    as the output from the reconstruction of the subsequent layer, but it might also involve a
                    ‘pull-back’ constraint from the preceding layer. Training data can be reconstructed when we
                    solve the problem backward from the output of the network through each layer. Based on this
                    formulation, we are able to attribute the potential leakage of the training data in a deep
                    network to its architecture. We also propose a metric to measure the level of security of a deep
                    learning model against gradient-based attacks on the training data.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Abhinav Aggarwal, Shiva Kasiviswanathan, Zekun Xu, Oluwaseyi Feyisetan, Nathanael Teissier
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs4" class="paper-title">
                        Reconstructing Test Labels from Noisy Loss Scores (Extended Abstract)
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=F1PR6BmamDx" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot B1]</a>
                </div>
                <div id="abs4" class="panel-footer panel-paper-footer collapse">
                    Label inference was recently introduced as the problem of reconstructing the ground truth labels
                    of a private dataset from just the (possibly perturbed) cross-entropy loss scores evaluated at
                    carefully crafted prediction vectors. In this paper, we generalize this result to provide
                    necessary and sufficient conditions under which label inference is possible from a broad class
                    of loss functions. We show that for many commonly used loss functions, including linearly
                    decomposable losses, some Bregman divergence-based losses and when common activation functions
                    are used, it is possible to design such attacks for arbitrary noise levels. We demonstrate that
                    these attacks can also be carried out through a lightweight augmentation to any neural network
                    model, enabling the adversary to make these attacks look benign. Our results call to attention
                    these vulnerabilities which might be currently under silent exploitation. Armed with this
                    information, individuals and organizations, which vend these seemingly innocuous aggregate
                    metrics from their classification models, can grasp the potential scope of the resulting
                    information leakage.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Ahmed Roushdy Elkordy, Saurav Prakash, Salman Avestimehr
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs5" class="paper-title">
                        Basil: A Fast and Byzantine-Resilient Approach for Decentralized Training
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=_vj5wbUcgRB" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot C0]</a>
                </div>
                <div id="abs5" class="panel-footer panel-paper-footer collapse">
                    Decentralized (i.e., serverless) learning across a large number of distributed nodes (e.g., mobile
                    users) has seen a surge of recent interests. The key advantage of these setups is that they
                    provide privacy for the local data of the users while not requiring a server for coordinating
                    the training. They can, however, suffer substantially from potential Byzantine nodes in the
                    network who can degrade the training performance. Detection and mitigation of Byzantine behaviors
                    in a decentralized learning setting is a daunting task, especially when the data distribution at
                    the users is heterogeneous. As our main contribution, we propose \texttt{Basil}, a fast and
                    computationally efficient Byzantine robust algorithm for decentralized training systems, which
                    leverages a novel sequential, memory assisted and performance based criteria for training over
                    a logical ring while filtering the Byzantine users. In the IID dataset distribution setting, we
                    provide the theoretical convergence guarantees of \texttt{Basil}, demonstrating its linear
                    convergence rate. Furthermore, for the IID setting, we experimentally demonstrate that
                    \texttt{Basil} is robust to various Byzantine attacks, including the strong Hidden attack,
                    while providing up to 16% higher test accuracy over the state-of-the-art Byzantine-resilient
                    decentralized learning approach. Additionally, we generalize \texttt{Basil} to the non-IID
                    dataset distribution setting by proposing Anonymous Cyclic Data Sharing (ACDS), a technique that
                    allows each node to anonymously share a random fraction of its local non-sensitive dataset
                    (e.g., landmarks images) with all other nodes. We demonstrate that \texttt{Basil} alongside
                    ACDS with only 5% data sharing provides effective toleration of Byzantine nodes, unlike the
                    state-of-the-art Byzantine robust algorithm that completely fails in the heterogeneous data setting.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        César Sabater, Jan Ramon
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs6" class="paper-title">
                        Zero Knowledge Arguments for Verifiable Sampling
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=YS47RZFtKm" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot C1]</a>
                </div>
                <div id="abs6" class="panel-footer panel-paper-footer collapse">
                    In privacy-preserving machine learning, it is less obvious to verify correct behavior of
                    participants
                    because they are not supposed to reveal their inputs in cleartext to other participants. It is
                    hence important to make federated machine learning robust against data poisoning and related
                    attacks. While input data can be related to a distributed ledger (blockchain), a less studied
                    input is formed by the random sampling parties perform. In this paper, we describe strategies
                    based on zero knowledge proofs to allow parties to prove they perform sampling (and other
                    computations) correctly. We sketch a number of alternative ways to implement our idea and
                    provide some preliminary experimental results.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Ossi Räisä, Antti Koskela, Antti Honkela
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs7" class="paper-title">
                        Differentially Private Hamiltonian Monte Carlo
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=oqNqsnOVwlK" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot C2]</a>
                </div>
                <div id="abs7" class="panel-footer panel-paper-footer collapse">
                    We present DP-HMC, a variant of Hamiltonian Monte Carlo (HMC) that is differentially private (DP).
                    We use the penalty algorithm of Yildirim and Ermis to make the acceptance test private, and add
                    Gaussian noise to the gradients of the target distribution to make the HMC proposal private. Our
                    main contribution is showing that DP-HMC has the correct invariant distribution, and is ergodic.
                    We also compare DP-HMC with the existing penalty algorithm, as well as DP-SGLD and DP-SGNHT.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Alexandra Peste, Dan Alistarh, Christoph H Lampert
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs8" class="paper-title">
                        SSSE: Efficiently Erasing Samples from Trained Machine Learning Models
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=GRMKEx3kEo" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot C3]</a>
                </div>
                <div id="abs8" class="panel-footer panel-paper-footer collapse">
                    The availability of large amounts of user-provided data has been key to the success of machine
                    learning for many real-world tasks. Recently, an increasing awareness has emerged that users
                    should be given more control about how their data is used. In particular, users should have the
                    right to prohibit the use of their data for training machine learning systems, and to have it
                    erased from already trained systems. While several sample erasure methods have been proposed,
                    all of them have drawbacks which have prevented them from gaining widespread adoption. In this
                    paper, we propose an efficient and effective algorithm, SSSE, for samples erasure that is
                    applicable to a wide class of machine learning models. From a second-order analysis of the
                    model's loss landscape we derive a closed-form update step of the model parameters that only
                    requires access to the data to be erased, not to the original training set. Experiments on
                    CelebFaces attributes (CelebA) and CIFAR10, show that in certain cases SSSE can erase samples
                    almost as well as the optimal, yet impractical, gold standard of training a new model from
                    scratch with only the permitted data.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                       Borja Balle, Giovanni Cherubin, Jamie Hayes
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs9" class="paper-title">
                        Reconstructing Training Data with Informed Adversaries
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=Yi2DZTbnBl4" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot D0]</a>
                </div>
                <div id="abs9" class="panel-footer panel-paper-footer collapse">
                    Given access to a machine learning model, can an adversary reconstruct the model’s training data?
                    This work proposes a formal threat model to study this question, shows that reconstruction
                    attacks are feasible in theory and in practice, and presents preliminary results assessing how
                    different factors of standard machine learning pipelines affect the success of reconstruction.
                    Finally, we empirically evaluate what levels of differential privacy suffice to prevent
                    reconstruction attacks.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Felix Morsbach, Tobias Dehling, Ali Sunyaev
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs10" class="paper-title">
                        Architecture Matters: Investigating the Influence of Differential Privacy on Neural Network
                        Design
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=Hp8sjc8LCwO" class="link-paper">[openreview]</a>
                    <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=17&spawny=43" class="link-paper">[Visit Poster at Spot D1]</a>
                </div>
                <div id="abs10" class="panel-footer panel-paper-footer collapse">
                    We explore the relationship between neural network architectures and model accuracy under
                    differential
                    privacy constraints. Our findings show that architectures that perform well without differential
                    privacy, do not necessarily do so with differential privacy. This shows that extant knowledge on
                    neural network architecture design cannot be seamlessly translated into the differential privacy
                    context. Moreover, as neural architecture search consumes privacy budget, future research is
                    required to better understand the relationship between neural network architectures and model
                    accuracy to enable better architecture design choices under differential privacy constraints.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Oualid Zari, Chuan Xu, Giovanni Neglia
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs12" class="paper-title">
                        Efficient passive membership inference attack in federated learning
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=LIBdHzDaRTb" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot E3]</a>
                </div>
                <div id="abs12" class="panel-footer panel-paper-footer collapse">
                    In cross-device federated learning (FL) setting, clients such as mobiles cooperate with the server
                    to train a global machine learning model, while maintaining their data locally. However, recent
                    work shows that client's private information can still be disclosed to an adversary who just
                    eavesdrops the messages exchanged between the client and the server. For example, the adversary
                    can infer whether the client owns a specific data instance, which is called a passive membership
                    inference attack. In this paper, we propose a new passive inference attack that requires much
                    less computation power and memory than existing methods. Our empirical results show that our attack
                    achieves a higher accuracy on CIFAR100 dataset (mora than 4 percentage points) with three orders of
                    magnitude less memory space and five orders of magnitude less calculations.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                       Borja Rodríguez Gálvez, Filip Granqvist, Rogier van Dalen, Matt Seigel
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs13" class="paper-title">
                        Enforcing fairness in private federated learning via the modified method of differential
                        multipliers
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=V2M0aUSguUC" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot F0]</a>
                </div>
                <div id="abs13" class="panel-footer panel-paper-footer collapse">
                    Federated learning with differential privacy, or private federated learning, provides a strategy to
                    train
                    machine learning models while respecting users’ privacy. However, differential privacy can
                    disproportionately degrade the performance of the models on under-represented groups, as these
                    parts of the distribution are difficult to learn in the presence of noise. Existing approaches
                    for enforcing fairness to machine learning models have considered the centralized setting, in
                    which the algorithm has access to the users’ data. This paper introduces an algorithm to enforce
                    group fairness in private federated learning, where users’ data does not leave their devices.
                    First, the paper extends the modified method of differential multipliers to empirical risk
                    minimization with fairness constraints, thus providing an algorithm to enforce fairness in
                    the central setting. Then, this algorithm is extended to the private federated learning
                    setting. The proposed algorithm, FPFL, is tested on a federated version of the Adult dataset
                    and an “unfair” version of the FEMNIST dataset. The experiments on these datasets show how
                    private federated learning accentuates unfairness in the trained models, and how FPFL is able
                    to mitigate such unfairness.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Xinyu Tang, Saeed Mahloujifar, Liwei Song, Virat Shejwalkar, Milad Nasr, Amir Houmansadr, Prateek Mittal
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs14" class="paper-title">
                        A Novel Self-Distillation Architecture to Defeat Membership Inference Attacks
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=eKkpcdSA52W" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot F3]</a>
                </div>
                <div id="abs14" class="panel-footer panel-paper-footer collapse">
                    Membership inference attacks are a key measure to evaluate privacy leakage in machine learning (ML)
                    models,
                    which aim to distinguish training members from non-members by exploiting differential behavior of
                    the models on member and non-member inputs. We propose a new framework to train privacy-preserving
                    models that induces similar behavior on member and non-member inputs to mitigate practical
                    membership
                    inference attacks. Our framework, called SELENA, has two major components. The first component
                    and the core of our defense, called Split-AI, is a novel ensemble architecture for training.
                    We prove that our Split-AI architecture defends against a large family of membership inference
                    attacks, however, it is susceptible to new adaptive attacks. Therefore, we use a second component
                    in our framework called Self-Distillation to protect against such stronger attacks, which
                    (self-)distills
                    the training dataset through our Split-AI ensemble and has no reliance on external public
                    datasets. We perform extensive experiments on major benchmark datasets and the results show that
                    our approach achieves a better trade-off between membership privacy and utility compared to
                    previous defenses.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        YUEFENG PENG
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs15" class="paper-title">
                        Unsupervised Membership Inference Attacks Against Machine Learning Models
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=lvjmpl00jqF" class="link-paper">[openreview]</a>
                      <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=108" class="link-paper">[Visit Poster at Spot H0]</a>
                </div>
                <div id="abs15" class="panel-footer panel-paper-footer collapse">
                    As a form of privacy leakage for machine learning (ML), membership inference (MI) attacks aim to
                    infer
                    whether given data samples have been used to train a target ML model. Existing state-of-the-art
                    MI attacks in black-box settings adopt a so-called shadow model to perform transfer attacks.
                    Such attacks achieve high inference accuracy but have many adversarial assumptions, such as having
                    a dataset from the same distribution as the target model’s training data and knowledge of the
                    target model structure. We propose a novel MI attack, called UMIA, which probes the target model
                    in an unsupervised way without any shadow model. We relax all the adversarial assumptions above,
                    demonstrating that MI attacks are applicable without any knowledge about the target model and
                    its training set. We empirically show that, with far fewer adversarial assumptions and computational
                    resources, UMIA can perform on bar with the state-of-the-art supervised MI attack.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                       Terrance Liu, Giuseppe Vietri, Steven Wu
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs16" class="paper-title">
                        Iterative Methods for Private Synthetic Data: Unifying Framework and New Methods
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=XOHcg2kgpVG" class="link-paper">[openreview]</a>
                      <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=108" class="link-paper">[Visit Poster at Spot G0]</a>
                </div>
                <div id="abs16" class="panel-footer panel-paper-footer collapse">
                    We study private synthetic data generation for query release, where the goal is to construct a
                    sanitized version of a sensitive dataset, subject to differential privacy, that approximately
                    preserves the answers to a large collection of statistical queries. We first present an algorithmic
                    framework that unifies a long line of iterative algorithms in the literature. Under this framework,
                    we propose two new methods. The first method, private entropy projection (PEP), can be viewed as
                    an advanced variant of MWEM that adaptively reuses past query measurements to boost accuracy. Our
                    second method, generative networks with the exponential mechanism (GEM), circumvents computational
                    bottlenecks in algorithms such as MWEM and PEP by optimizing over generative models parameterized
                    by neural networks, which capture a rich family of distributions while enabling fast gradient-based
                    optimization. We demonstrate that PEP and GEM empirically outperform existing algorithms.
                    Furthermore, we show that GEM nicely incorporates prior information from public data while
                    overcoming limitations of PMW^Pub, the existing state-of-the-art method that also leverages public
                    data.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Cecilia Ferrando, Jennifer Gillenwater, Alex Kulesza
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs17" class="paper-title">
                        Combining Public and Private Data
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=CbArlTV4tr" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot G1]</a>
                </div>
                <div id="abs17" class="panel-footer panel-paper-footer collapse">
                    Differential privacy is widely adopted to provide provable privacy guarantees in data analysis. We
                    consider the problem of combining public and private data (and, more generally, data with
                    heterogeneous privacy needs) for estimating aggregate statistics. We introduce a mixed estimator
                    of the mean optimized to minimize the variance. We argue that our mechanism is preferable to
                    techniques that preserve the privacy of individuals by subsampling data proportionally to the
                    privacy needs of users. Similarly, we present a mixed median estimator based on the exponential
                    mechanism. We compare our mechanisms to the methods proposed in Jorgensen et al. [2015]. Our
                    experiments provide empirical evidence that our mechanisms often outperform the baseline methods.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Arpita Patra, Thomas Schneider, Ajith Suresh, Hossein Yalame
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs19" class="paper-title">
                        ABY2.0: New Efficient Primitives for STPC with Applications to Privacy in Machine Learning
                        (Extended Abstract)
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=hrV8Tn5w5D0" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot G2]</a>
                </div>
                <div id="abs19" class="panel-footer panel-paper-footer collapse">
                    In this work, we improve semi-honest secure two-party computation (STPC) over rings, specially for
                    privacy-preserving machine learning, with a focus on the efficiency of the online phase. We
                    construct
                    efficient protocols for several privacy-preserving machine learning (PPML) primitives such as scalar
                    product, matrix multiplication, ReLU, and maxpool. The online communication of our scalar product
                    is two ring elements {\em irrespective} of the vector dimension, which is a feature achieved for
                    the first time in PPML literature. We implement and benchmark training and inference of Logistic
                    Regression and Neural Networks over LAN and WAN networks. For training, we improve online runtime
                    (both for LAN and WAN) over SecureML (Mohassel et al., IEEE S\&P'17) in the range 1.5&times;--6.1&times;,
                    while for
                    inference, the improvements are in the range of 2.5&times;--754.3&times;.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Virat Shejwalkar, Huseyin A Inan, Amir Houmansadr, Robert Sim
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs20" class="paper-title">
                        Membership Inference Attacks Against NLP Classification Models
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=74lwg5oxheC" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot H0]</a>
                </div>
                <div id="abs20" class="panel-footer panel-paper-footer collapse">
                    The success of natural language processing (NLP) is making NLP applications commonplace.
                    Unfortunately,
                    recent research has shown that privacy might be at stake given that these models are often trained
                    on private user data. While privacy risks are demonstrated in text generation settings, privacy
                    risks of the text classification settings, which subsume myriad downstream applications, are largely
                    unexplored. In this work, we study the susceptibility of NLP classification models, used for text
                    classification tasks, to membership inference (MI), which is a fundamental type of privacy leakage.
                    We design a comprehensive suite of attacks to assess the risk of sample-level MI, as well as that
                    of relatively unexplored user-level MI. We introduce novel user-level MI attacks that outperform
                    the existing attacks and conduct experiments on Transformer-based and RNN-based NLP models. Our
                    evaluations show that user-level MI is significantly stronger than sample-level MI. We further
                    perform in-depth analyses showing the effect of various NLP-specific parameters on MI against
                    NLP classification models.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Yongqin Wang, Edward Suh, Wenjie Xiong, Brian Knott, Benjamin Lefaudeux, Murali Annavaram, Hsien-Hsin Lee
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs21" class="paper-title">
                        Characterizing and Improving MPC-based Private Inference for Transformer-based Models
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=81IVQXVoi-Y" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot H2]</a>
                </div>
                <div id="abs21" class="panel-footer panel-paper-footer collapse">
                    Secure multi-party computation (MPC) is gaining popularity with the growing demand for
                    privacy-preserving
                    cloud services. While there has been plenty of attention to MPCs for convolution neural networks
                    (CNNs),
                    MPC-based private inference for Transformer models has not been studied in detail. This paper
                    provides
                    a characterization study of the performance overhead for running Transformer models with secure MPC,
                    and proposes an optimization for embedding tables. Our study shows that Transformers introduce a
                    couple of new challenges for MPC-based private inference: softmax and embedded tables. To address
                    the overhead of embedding table accesses under MPC, we propose to use tensor-train (TT)
                    decomposition,
                    a mechanism that splits a large embedding tables into multiple smaller embedding tables. For the
                    NLP workloads, the experiments show that the TT decomposition can speed up embedding table accesses
                    by 2x with only a 1.19 drop in the masked-language model perplexity score.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Shubham Jain, Ana-Maria Cretu, Yves-Alexandre de Montjoye
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs22" class="paper-title">
                        Adversarial Detection Avoidance Attacks: Evaluating the robustness of perceptual hashing-based
                        client-side scanning
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=CQbqeGAM_Ki" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot H3]</a>
                </div>
                <div id="abs22" class="panel-footer panel-paper-footer collapse">
                    End-to-end encryption (E2EE) in messaging platforms enables people to securely and privately
                    communicate
                    with one another. Its widespread adoption however raised concerns that illegal content might now
                    be shared undetected. Client-side scanning based on perceptual hashing has been recently proposed
                    by governments and researchers to detect illegal content in E2EE communications. We propose the
                    first framework to evaluate the robustness of perceptual hashing-based client-side scanning to
                    detection avoidance attacks and show current systems to not be robust. We propose three adversarial
                    attacks---a general black-box attack and two white-box attacks for discrete cosine transform-based
                    algorithms--against perceptual hashing algorithms. In a large-scale evaluation, we show perceptual
                    hashing-based client-side scanning mechanisms to be highly vulnerable to detection avoidance attacks
                    in a black-box setting, with more than 99.9\% of images successfully attacked while preserving the
                    content of the image. We further show several mitigation strategies, such as expanding the database
                    with hashes of images modified using our attack, or increasing the detection threshold, to be
                    ineffective against our attack. Taken together, our results shed serious doubts on the robustness
                    of perceptual hashing-based client-side scanning mechanisms currently proposed by governments,
                    organizations, and researchers around the world.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Andres Munoz medina, Matthew Joseph, Jennifer Gillenwater, Mónica Ribero
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs23" class="paper-title">
                        A Joint Exponential Mechanism for Differentially Private Top-k Set
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=BjBeRB3NqG" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot I0]</a>
                </div>
                <div id="abs23" class="panel-footer panel-paper-footer collapse">
                    We present a novel differentially private algorithm for releasing the set of k elements with the
                    highest
                    counts from a data domain of d elements. We define a ``joint'' instance of the exponential mechanism
                    (EM) whose output space consists of all O(d^k) size-k subsets; yet, we are able to show how to
                    sample
                    from this EM in only time O(dk^3). Experiments suggest that this joint approach can yield utility
                    improvements over the existing state of the art for small problem sizes.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Karan Chadha, John Duchi, Rohith Kuditipudi
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs24" class="paper-title">
                        Private Confidence Sets
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=aU7uUaQHTI-" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot I1]</a>
                </div>
                <div id="abs24" class="panel-footer panel-paper-footer collapse">
                    We consider statistical inference under privacy constraints. In particular, we give differentially
                    private algorithms for estimating coverage probabilities and computing valid confidence sets,
                    and prove upper bounds on the error of our estimates and the length of our confidence sets. Our
                    bounds apply to broad classes of data distributions and statistics of interest, and for fixed
                    we match the higher-order asymptotic accuracy of the standard (non-private) non-parametric
                    bootstrap.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Sen Yuan, Milan Shen, Ilya Mironov, Anderson Nascimento
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs25" class="paper-title">
                        Label Private Deep Learning Training based on Secure Multiparty Computation and Differential
                        Privacy
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=tg9W8YAJVO6" class="link-paper">[openreview]</a>
                    <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=108" class="link-paper">[Visit Poster at Spot I2]</a>
                </div>
                <div id="abs25" class="panel-footer panel-paper-footer collapse">
                    Secure Multiparty Computation (MPC) is an invaluable tool for training machine learning models when
                    the training data cannot be directly accessed by the model trainer. Unfortunately, complex
                    algorithms,
                    such as deep learning models, have their computational complexities increased by orders of magnitude
                    when performed using MPC protocols. In this contribution, we study how to efficiently train an
                    important class of machine learning problems by using MPC where features are known by one of the
                    computing parties and only the labels are private. We propose new protocols combining differential
                    privacy (DP) and MPC in order to privately and efficiently train a deep learning model in such
                    scenario. More specifically, we release differentially private information during the MPC
                    computation to dramatically reduce the training time. All released information idoes not compromise
                    the privacy of the labels at the individual level. Our protocols can have running times that are
                    orders of magnitude better than a straightforward use of MPC at a moderate cost in model accuracy.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Margarita Vinaroz, Mijung Park, Mijung Park
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs26" class="paper-title">
                        DP-SEP: Differentially private stochastic expectation propagation
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=lY9q2kDVQVF" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot I3]</a>
                </div>
                <div id="abs26" class="panel-footer panel-paper-footer collapse">
                    We are interested in privatizing an approximate posterior inference algorithm called Expectation
                    Propagation (EP). EP approximates the posterior by iteratively refining approximations to the
                    local likelihoods, and is known to provide better posterior uncertainties than those by variational
                    inference. However, using EP for large-scale datasets imposes a challenge in terms of memory
                    requirements as it needs to maintain each of the local approximates in memory. To overcome this
                    problem, stochastic expectation propagation (SEP) was proposed, which only considers a unique
                    local factor that captures the average effect of each likelihood term to the posterior and refines
                    it in a way analogous to EP. Therefore in this work, we focus on developing a differentially
                    private stochastic expectation propagation(DP-SEP) algorithm, which outputs differentially
                    private natural parameters of the exponential-family posteriors in each step of SEP.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                       Rachel Cummings, Vitaly Feldman, Audra McMillan, Kunal Talwar
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs27" class="paper-title">
                        Mean Estimation with User-level Privacy under Data Heterogeneity
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=oYbQDV3mon-" class="link-paper">[openreview]</a>
                    <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=4&spawny=121" class="link-paper">[Visit Poster at Spot J0]</a>
                </div>
                <div id="abs27" class="panel-footer panel-paper-footer collapse">
                    A key challenge for data analysis in the federated setting is that user data is heterogeneous, i.e.,
                    it cannot be assumed to be sampled from the same distribution. Further, in practice, different
                    users may possess vastly different number of samples. In this work we propose a simple model of
                    heterogeneous user data that differs in both distribution and quantity of data, and we provide a
                    method for estimating the population-level mean while preserving user-level differential privacy.
                    We demonstrate near asymptotic optimality of our estimator among nearly unbiased estimators.
                    In particular, while the optimal non-private estimator can be shown to be linear, we show that
                    privacy
                    constrains us to use a non-linear estimator.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Nitin Agrawal, James Bell, Adrià Gascón, Matt Kusner
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs28" class="paper-title">
                        Certified Predictions using MPC-Friendly Publicly Verifiable Covertly Secure Commitment
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=8LZnVuYY-ZG" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=17&spawny=121" class="link-paper">[Visit Poster at Spot J1]</a>
                </div>
                <div id="abs28" class="panel-footer panel-paper-footer collapse">
                    We address the problem of efficiently and securely enabling certified predictions on deep learning
                    . This addresses the scenario where a party P1 owns a confidential model that has been certified
                    by an authority to have a certain property e.g. fairness. Subsequently, another party P2 wants
                    to perform a prediction on the model with an assurance that the certified model was used. We
                    present a solution for this problem based on MPC commitments. Our constructions operate in the
                    publicly verifiable covert (PVC) security model, which is a relaxation of the malicious model of
                    MPC, appropriate in settings where P1 faces a reputational harm if caught cheating. We introduce
                    the notion of a PVC commitment scheme and indexed hash functions to build commitment schemes
                    tailored to the PVC framework, and propose constructions for both arithmetic and Boolean circuits
                    that result in very efficient circuits. From a practical standpoint, our constructions for Boolean
                    circuits are 60x faster to evaluate securely, and use 36x less communication than baseline methods
                    based on hashing. Moreover, we show that our constructions are tight in terms of required non-linear
                    operations, and present a technique to amplify the security properties of our constructions that
                    allows to efficiently recover malicious guarantees with statistical security.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Edward Vendrow, Joshua Vendrow
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs29" class="paper-title">
                        Realistic Face Reconstruction from Deep Embeddings
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=-WsBmzWwPee" class="link-paper">[openreview]</a>
                    <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=121" class="link-paper">[Visit Poster at Spot J2]</a>
                </div>
                <div id="abs29" class="panel-footer panel-paper-footer collapse">
                    Modern face recognition systems use deep convolution neural networks to extract latent embeddings
                    from
                    face images. Since basic arithmetic operations on embeddings are needed to make comparisons,
                    generic encryption schemes cannot be used. This leaves facial embedding unprotected and susceptible
                    to privacy attacks that reconstruction facial identity. We propose a search algorithm on the latent
                    vector space of StyleGAN to find a matching face. Our process yields latent vectors that generate
                    face images that are high-resolution, realistic, and reconstruct relevant attributes of the original
                    face. Further, we demonstrate that our process is capable of fooling FaceNet, a state-of-the-art
                    face recognition system.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Wei-Ning Chen, Christopher A. Choquette-Choo, Peter Kairouz
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs30" class="paper-title">
                        Communication Efficient Federated Learning with Secure Aggregation and Differential Privacy
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=NqikEZ6ABea" class="link-paper">[openreview]</a>
                    <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=43&spawny=121" class="link-paper">[Visit Poster at Spot J3]</a>
                </div>
                <div id="abs30" class="panel-footer panel-paper-footer collapse">
                    Optimizing the \puc tradeoff is a key challenge for federated learning. Under distributed
                    differential privacy
                    (DP) via secure aggregation (SecAgg), we prove that the worst-case communication cost per client
                    must be
                    at least &Omega;(dlog(n<sup>2</sup>&epsilon;<sup>2</sup>/d))
                    to achieve O(d/(n<sup>2</sup>&epsilon;<sup>2</sup>)) centralized error, which matches the error
                    under
                    central DP. Despite this bound, we leverage the near-sparse structure of model updates, evidenced
                    through recent empirical studies, to obtain improved tradeoffs for distributed \DP. In particular,
                    we leverage linear compression methods, namely sketching, to attain compression rates of up to 50&times;
                    with no significant decrease in model test accuracy achieving a noise multiplier 0.5.
                    Our work demonstrates that fundamental tradeoffs in differentially private federated learning can
                    be drastically improved in practice.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Dmitrii Usynin, Alexander Ziller, Moritz Knolle, Daniel Rueckert, Georgios Kaissis
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs31" class="paper-title">
                        An automatic differentiation system for the age of differential privacy
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=_b0jOHfZbfD" class="link-paper">[openreview]</a>
                    <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=4&spawny=4" class="link-paper">[Visit Poster at Spot A0]</a>
                </div>
                <div id="abs31" class="panel-footer panel-paper-footer collapse">
                    We introduce Tritium, an automatic differentiation-based sensitivity analysis framework for
                    differentially
                    private (DP) machine learning (ML). Optimal noise calibration in this setting requires efficient
                    Jacobian
                    matrix computations and tight bounds on the L2-sensitivity. Our framework achieves these objectives
                    by
                    relying on a functional analysis-based method for sensitivity tracking, which we briefly outline.
                    This approach interoperates naturally and seamlessly with static graph-based automatic
                    differentiation,
                    which enables order-of-magnitude improvements in compilation times compared to previous work.
                    Moreover,
                    we demonstrate that optimising the sensitivity of the entire computational graph at once yields
                    substantially tighter estimates of the true sensitivity compared to interval bound propagation
                    techniques.
                    Our work naturally befits recent developments in DP such as individual privacy accounting, aiming
                    to offer improved privacy-utility trade-offs, and represents a step towards the integration of
                    accessible machine learning tooling with advanced privacy accounting systems.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Ashkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani Malek, John Nguyen, Sayan Ghosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, Ilya Mironov
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs32" class="paper-title">
                        Opacus: User-Friendly Differential Privacy Library in PyTorch
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=EopKEYBoI-" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=4&spawny=121" class="link-paper">[Visit Poster at Spot D3]</a>
                </div>
                <div id="abs32" class="panel-footer panel-paper-footer collapse">
                    We introduce Opacus, a free, open-source PyTorch library for training deep learning models with
                    differential
                    privacy (hosted at https://opacus.ai). Opacus is designed for simplicity, flexibility, and speed. It
                    provides a simple and user-friendly API, and enables machine learning practitioners to make a
                    training
                    pipeline private by adding as little as two lines to their code. It supports a wide variety of
                    layers,
                    including multi-head attention, convolution, LSTM, and embedding, right out of the box, and it also
                    provides
                    the means for supporting other user-defined layers. Opacus computes batched per-sample gradients,
                    providing
                    better efficiency compared to the traditional “micro batch” approach. In this paper we present
                    Opacus,
                    detail the principles that drove its implementation and unique features, and compare its performance
                    against other frameworks for differential privacy in ML.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Lennart Braun, Rosario Cammarota, Thomas Schneider
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs33" class="paper-title">
                        A Generic Hybrid 2PC Framework with Application to Private Inference of Unmodified Neural
                        Networks (Extended Abstract)
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=CXFh9utHuw2" class="link-paper">[openreview]</a>
                    <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=43&spawny=82" class="link-paper">[Visit Poster at Spot G3]</a>
                </div>
                <div id="abs33" class="panel-footer panel-paper-footer collapse">
                    We present a new framework for generic mixed-protocol secure two-party computation (2PC) and private
                    evaluation of neural
                    networks based on the recent MOTION framework (Braun et al., ePrint '20). We implement five
                    different 2PC protocols in the
                    semi-honest setting -- Yao's garbled circuits, arithmetic and Boolean variants of
                    Goldreich-Micali-Wigderson (GMW), and
                    two secret-sharing-based protocols from ABY2.0 (Patra et al., USENIX Security '21) -- together with
                    20 conversions among
                    each other and new optimizations. We explore the feasibility of evaluating neural networks with 2PC
                    without making modifications
                    to their structure, and provide secure tensor data types and specialized building blocks for common
                    tensor operations. By
                    supporting the Open Neural Network Exchange (ONNX) file format, this yields an easy-to-use solution
                    for privately evaluating
                    neural networks, and is interoperable with industry-standard deep learning frameworks such as
                    TensorFlow and PyTorch. By exploiting
                    the networks' high-level structure and using common 2PC techniques, we obtain a performance that is
                    comparable to that of recent,
                    highly optimized works and significantly better than when using generic 2PC for low-level hybrid
                    circuits.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Jordan Awan, Vinayak Rao
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs34" class="paper-title">
                        Privacy-Aware Rejection Sampling
                        <font color="#d07200"><b>(oral)</b></font>
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=E59HmNab0CB" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot B2]</a>
                </div>
                <div id="abs34" class="panel-footer panel-paper-footer collapse">
                    Differential privacy (DP) offers strong protection against adversaries with arbitrary
                    side-information
                    and computational power. However, many implementations of DP mechanisms leave themselves vulnerable
                    to side channel attacks, such as timing attacks. As many privacy mechanisms, such as the exponential
                    mechanism, do not lend themselves to easy implementations, when sampling methods such as MCMC or
                    rejection sampling are used, the runtime can leak privacy. In this work, we quantify the privacy
                    cost due to the runtime of a rejection sampler in terms of (&epsilon;, &delta;)-DP. We also propose
                    three modifications
                    to the rejection sampling algorithm, to protect against timing attacks by making the runtime
                    independent
                    of the data. We also use our techniques to develop an adaptive rejection sampler for log-Holder
                    densities,
                    which also has data-independent runtime.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Jordan Awan, Salil Vadhan
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs35" class="paper-title">
                        Canonical Noise Distributions and Private Hypothesis Tests
                        <font color="#d07200"><b>(oral)</b></font>
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=pr_6JwMBLa1" class="link-paper">[openreview]</a>
                      <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=43&spawny=17" class="link-paper">[Visit Poster at Spot B3]</a>
                </div>
                <div id="abs35" class="panel-footer panel-paper-footer collapse">
                    In the setting of f-DP, we propose the concept \emph{canonical noise distribution} (CND) which
                    captures
                    whether an additive privacy mechanism is tailored for a given f, and give a construction of a CND
                    for an arbitrary tradeoff function . We show that private hypothesis tests are intimately related
                    to CNDs, allowing for the release of private p-values at no additional privacy cost as well as the
                    construction of uniformly most powerful (UMP) tests for binary data. We apply our techniques to
                    difference of proportions testing.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Amir Mohammad Abouei, Clement Louis Canonne
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs36" class="paper-title">
                        Differential Privacy via Group Shuffling
                        <font color="#d07200"><b>(oral)</b></font>
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=j7WJc7-VKZM" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot D2]</a>
                </div>
                <div id="abs36" class="panel-footer panel-paper-footer collapse">
                    The past decade has seen data privacy emerge as a fundamental and pressing issue. Among the tools
                    developed
                    to tackle it, differential privacy has emerged as a central and principled framework, with specific
                    variants capturing various threat models. In particular, the recently proposed shuffle model of
                    differential privacy allows for promising tradeoffs between accuracy and privacy. However, the
                    shuffle
                    model may not be suitable in all situations, as it relies on a distributed setting where all users
                    can coordinate and trust (or simulate) a joint shuffling algorithm.
                    To address this, we introduce a new model, the group shuffle model, in which users are partitioned
                    into several groups, each group having its own local shuffler. We investigate the privacy/accuracy
                    tradeoffs in our model, by comparing it to both the shuffle and local models of privacy, which it
                    some sense interpolates between. In addition to general relations between group shuffle, shuffle,
                    and local privacy, we provide a detailed comparison of the cost and benefit of the group shuffle
                    model, by providing both upper and lower bounds for the specific task of binary summation.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Xuechen Li, Florian Tramer, Percy Liang, Tatsunori Hashimoto
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs37" class="paper-title">
                        Simple Baselines Are Strong Performers for Differentially Private Natural Language Processing
                        <font color="#d07200"><b>(oral)</b></font>
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=oOiSJEr2-Tt" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot E1]</a>
                </div>
                <div id="abs37" class="panel-footer panel-paper-footer collapse">
                    Differentially private learning has seen limited success for deep learning models of text, resulting
                    in a perception that differential privacy may be incompatible with the language model fine-tuning
                    paradigm. We demonstrate that this perception is inaccurate and that with the right setup, high
                    performing private models can be learned on moderately-sized corpora by directly fine-tuning with
                    differentially private optimization. Our work highlights the important role of hyperparameters,
                    task formulations, and pretrained models. Our analyses also show that the low performance of naive
                    differentially private baselines in prior work is attributable to suboptimal choices in these
                    factors.
                    Empirical results reveal that differentially private optimization does not suffer from
                    dimension-dependent performance
                    degradation with pretrained models and achieves performance on-par with state-of-the-art private
                    training
                    procedures and strong non-private baselines.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Robert Istvan Busa-Fekete, Andres Munoz medina, Umar Syed, Sergei Vassilvitskii
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs38" class="paper-title">
                        Population Level Privacy Leakage in Binary Classification wtih Label Noise
                        <font color="#d07200"><b>(oral)</b></font>
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=Gf2EuAB9Xj" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot F2]</a>
                </div>
                <div id="abs38" class="panel-footer panel-paper-footer collapse">
                    We study the privacy limitations of label differential privacy. Label differential privacy has
                    emerged
                    as an intermediate trust model between local and central differential privacy, where only the label
                    of each training example is protected (and the features are assumed to be public). We show that the
                    guarantees provided by label DP are significantly weaker than they appear, as an adversary can
                    "un-noise" the perturbed labels. Formally we show that the privacy loss has a close connection
                    with Jeffreys' divergence of the conditional distribution between positive and negative labels,
                    which allows explicit formulation of the trade-off between utility and privacy in this setting.
                    Our results suggest how to select public features that optimize this trade-off. But we still show
                    that there is no free lunch --- instances where label differential privacy guarantees are strong
                    are exactly those where a good classifier does not exist. We complement the negative results with
                    a non-parametric estimator for the true privacy loss, and apply our techniques on large-scale
                    benchmark data to demonstrate how to achieve a desired privacy protection.
                </div>
            </div>
            <div class="panel panel-default panel-paper">
                <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                       Aditya Hegde, Helen Möllering, Thomas Schneider, Hossein Yalame
                        </span>
                    <br/>
                    <a data-toggle="collapse" href="#abs39" class="paper-title">
                        SoK: Privacy-preserving Clustering (Extended Abstract)
                        <font color="#d07200"><b>(oral)</b></font>
                    </a> &nbsp;&nbsp;
                    <a href="https://openreview.net/forum?id=UEtM6ioVa3" class="link-paper">[openreview]</a>
                     <a href="https://eventhosts.gather.town/MqvFFxrydV6Q5uEK/priml-2021?map=4x10-poster-room-with-exit-text&spawnx=30&spawny=43" class="link-paper">[Visit Poster at Spot H1]</a>
                </div>
                <div id="abs39" class="panel-footer panel-paper-footer collapse">
                    Clustering is a popular unsupervised machine learning technique that groups similar input elements
                    into clusters. In many applications, sensitive information is clustered that should not be leaked.
                    Moreover, nowadays it is often required to combine data from multiple sources to increase the
                    quality of the analysis as well as to outsource complex computation to powerful cloud servers.
                    This calls for efficient privacy-preserving clustering. In this work, we systematically analyze
                    the state-of-the-art in privacy-preserving clustering. We implement and benchmark today's four
                    most efficient fully private clustering protocols by Cheon et al. (SAC'19), Meng et al. (ArXiv'19),
                    Mohassel et al. (PETS'20), and Bozdemir et al. (ASIACCS'21) with respect to communication,
                    computation, and clustering quality.
                </div>
            </div>
        </div>
    </div>
</section>


<!-- Call for travel grants -->
<!--
    <section id="grants" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Travel Grants</h2>
                <p>
                Thanks to our generous sponsors, we are able to provide a limited number of travel grants of up to $800 to help partially cover the expenses of authors of accepted papers who have not received other travel support from NeurIPS this year.
                To apply, please send an email to <a href="mailto:ppml18@easychair.org?Subject=PPML18%20Travel%20Grant%20Application">ppml18@easychair.org</a> with the subject “PPML18 Travel Grant Application” including your resume and a half-page statement of purpose mentioning the title and the authors of your accepted paper and a summary of anticipated travel expenses. If you are an undergraduate or graduate student, we ask for a half-page recommendation letter supporting your application to be sent to us by the deadline. The deadline for applications is <b>November 11, 2018 (11:59pm AoE)</b>. The notifications will be sent by <b>November 16</b>. Please feel free to send us an email if you have any questions.
            </div>
        </div>
    </section>
-->

<!-- Organizers Section -->
<section id="organizers" class="content-section text-center">
    <div class="row">
        <div class="col-lg-8 col-lg-offset-2">
            <h2>Organization</h2>
            <br/>
            <h3>Workshop organizers</h3>
            <ul class="list-group">
                <li class="list-group-item organizer">Borja Balle (DeepMind)</li>
                <li class="list-group-item organizer">Giovanni Cherubin (Alan Turing Institute)</li>
                <li class="list-group-item organizer">Kamalika Chaudhuri (UC San Diego and Facebook AI Research)</li>
                <li class="list-group-item organizer">Antti Honkela (University of Helsinki)</li>
                <li class="list-group-item organizer">Jonathan Lebensold (Mila and McGill University)</li>
                <li class="list-group-item organizer">Casey Meehan (UC San Diego)</li>
                <li class="list-group-item organizer">Mijung Park (University of British Columbia)</li>
                <li class="list-group-item organizer">Yu-Xiang Wang (UC Santa Barbara)</li>
                <li class="list-group-item organizer">Adrian Weller (Alan Turing Institute & Cambridge University)</li>
                <li class="list-group-item organizer">Yuqing Zhu (UC Santa Barbara)</li>
            </ul>
            <br/>
            <h3>Program Committee</h3>
        </div>
    </div>
</section>

<!-- Footer -->
<footer>
    <div class="container text-center">
        <p>Sponsored by</p>
    </div>
</footer>

<!-- jQuery -->
<script src="js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="js/bootstrap.min.js"></script>

<!-- Plugin JavaScript -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>

<!-- Theme JavaScript -->
<script src="js/script.js"></script>

</body>

</html>
