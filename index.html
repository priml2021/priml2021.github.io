<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta property="og:image" content="https://priml2021.github.io/img/preview-image.png" />
    <meta property="og:image:alt" content="Privacy in Machine Learning (PriML) NeurIPS 2021 Workshop" />
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Privacy in Machine Learning (NeurIPS 2021 Workshop)" />
    <meta name="twitter:image" content="https://priml-workshop.github.io/img/twitter-img.png" />
    <meta name="twitter:image:alt" content="PriML Workshop" />
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Privacy in Machine Learning (NeurIPS 2021 Workshop)</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.min.css" rel="stylesheet">

    <link href="css/style.css" rel="stylesheet">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    Menu <i class="fa fa-bars"></i>
                </button>
                <a class="navbar-brand page-scroll" href="#page-top">
                    <span class="light">P<span style="text-transform:lowercase">ri</span>ML'19</span>
                </a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#about">Scope</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#dates">CFP &amp Dates</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#speakers">Invited Speakers</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#schedule">Schedule</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#papers">Accepted Papers</a>
                    </li>
                    <!-- <li>
                        <a class="page-scroll" href="#grants">Travel Grants</a>
                    </li> -->
                    <li>
                        <a class="page-scroll" href="#organizers">Organizers</a>
                    </li>
                    <li>
                      <a class="page-scroll" href="#access">Accessibility</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <!-- Intro Header -->
    <header class="intro">
        <div class="intro-body">
            <div class="container">
                <div class="row">
                    <div class="col-md-8 col-md-offset-2">
                        <h1 class="brand-heading">Privacy in Machine Learning</h1>
                        <p class="intro-text">
                            <a href="https://neurips.cc/" style="color:white">NeurIPS 2019</a> Workshop
                            <br />Vancouver, December 14
                        </p>
                        <p class="location-text">
                            <br /> Room: East MR 8 + 15
                            <!-- <br /> <a href="https://slideslive.com/38922114">View on SlidesLive</a> -->
                            <br /> <a href="#slideslive">View on SlidesLive</a>
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </header>

    <!-- About Section -->
    <section id="about" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Scope</h2>

                  <p>The goal of our workshop is to bring together privacy experts working in academia and
                  industry to discuss the present and the future of privacy-aware technologies powered by
                  machine learning. The workshop will focus on the technical aspects of privacy research and
                  deployment with invited and contributed talks by distinguished researchers in the area.
                  We encourage submissions exploring a broad range of research areas related to data privacy,
                  including but not limited to:</p>

                  <ul class="list-group">
                      <li class="list-group-item speaker">Differential privacy: theory, applications, and implementations</li>
                      <li class="list-group-item speaker">Privacy-preserving machine learning</li>
                      <li class="list-group-item speaker">Trade-offs between privacy and utility</li>
                      <li class="list-group-item speaker">Programming languages for privacy-preserving data analysis</li>
                      <li class="list-group-item speaker">Statistical and information-theoretic notions of privacy</li>
                      <li class="list-group-item speaker">Empirical and theoretical comparisons between different notions of privacy</li>
                      <li class="list-group-item speaker">Privacy attacks</li>
                      <li class="list-group-item speaker">Policy-making aspects of data privacy</li>
                      <li class="list-group-item speaker">Secure multi-party computation techniques for machine learning</li>
                      <li class="list-group-item speaker">Learning on encrypted data, homomorphic encryption</li>
                      <li class="list-group-item speaker">Distributed privacy-preserving algorithms</li>
                      <li class="list-group-item speaker">Privacy in autonomous systems</li>
                      <li class="list-group-item speaker">Online social networks privacy</li>
                      <li class="list-group-item speaker">Interplay between privacy and adversarial robustness in machine learning</li>
                      <li class="list-group-item speaker">Relations between privacy, fairness and transparency</li>
                  </ul>
            </div>
        </div>
    </section>

    <!-- CFP & Dates Section -->
        <section id="dates" class="container content-section text-center">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>Call For Papers &amp; Important Dates</h2>
                    <a href="cfp-priml19.txt" class="btn btn-default btn-lg">Download Full CFP</a>
                    <br/>
                    <br/>
                    <br/>
                    <p>
                        <b>Submission deadline</b>: September 9, 2019, 23:59 UTC
                        <br/><b>Notification of acceptance</b>: October 1, 2019
                        <br/><b>NeurIPS early <a href="https://nips.cc/Conferences/2019/Press?article=2299">registration</a> deadline</b>: October 23, 2019
                        <br/><b>Workshop</b>: December 14, 2019 (Saturday)
                    </p>
                    <h3>Instructions</h3>
                      <p>
                        The submission deadline has now passed.
                        If your submission was accepted for a poster presentation,
                        please make your posters 36W x 48H inches or 90 x 122 cm.
                        Posters should be on light weight paper and should not be laminated.
                        As you design your poster,
                        you may find the following resource helpful:
                        <a href="resources/accessibility_posters_gilson2019.pdf">Guidelines for Creating Accessible Printed Posters</a>.

                      </p>
                    <!-- <a href="https://easychair.org/conferences/?conf=priml2019" class="btn btn-default btn-lg">Submit Your Abstract</a> -->
                    <br/>
                    <br/>
                    <br/>
                    <h3>Related Workshops</h3>
                        <p><b><a href="http://federated-learning.org/fl-neurips-2019/">Federated Learning for Data Privacy and Confidentiality</a> @ NeurIPS</b>: December 13, 2019</p>
                </div>
            </div>
        </section>

    <!-- Speakers Section -->
    <section id="speakers" class="container content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Invited Speakers</h2>
                <ul class="list-group">
                    <li class="list-group-item speaker">Philip Leclerc (US Census)</li>
                    <li class="list-group-item speaker">Ashwin Machanavajjhala (Duke University)</li>
                    <li class="list-group-item speaker">Brendan McMahan (Google)</li>
                    <li class="list-group-item speaker">Lalitha Sankar (Arizona State University)</li>
                </ul>
            </div>
        </div>
    </section>

    <!-- Schedule Section -->
    <section id="schedule" class="container content-section text-center">
        <div class="row">
            <div class="col-sm-8 col-sm-offset-2">
                <h2>Schedule</h2>
                <table class="table schedule">
                    <tbody>
                    <tr>
                        <td class="time">8:10</td>
                        <td class="slot">Opening</td>
                    </tr>

                    <tr>
                        <td class="time">8:15</td>
                        <td class="slot talk">
                        Invited talk:
                        <a href="#tabs2" data-toggle="collapse" class="accordion-toggle">
                        Brendan McMahan
                        &mdash;
                        Privacy for Federated Learning, and Federated Learning for Privacy
                        </a>&nbsp;&nbsp;
                        <!-- <a href="slides/slides.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs2">
                            More details coming soon
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">9:05</td>
                        <td class="slot talk"><a href="#tabs3" data-toggle="collapse" class="accordion-toggle">
                        Gaussian Differential Privacy
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">
                        Jinshuo Dong, Aaron Roth and Weijie Su
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs3">
                              Differential privacy has seen remarkable success as a rigorous and practical formalization of data privacy in the past decade.
                              This privacy definition and its divergence based relaxations, however, have several acknowledged weaknesses, either in handling composition of private algorithms or in analyzing important primitives like privacy amplification by subsampling.
                              Inspired by the hypothesis testing formulation of privacy, this paper proposes a new relaxation, which we term &#10077;f-differential privacy&#10078; (f-DP).
                              This notion of privacy has a number of appealing properties and, in particular, avoids difficulties associated with divergence based relaxations.
                              First, f-DP preserves the hypothesis testing interpretation.
                              In addition, f-DP allows for lossless reasoning about composition in an algebraic fashion.
                              Moreover, we provide a powerful technique to import existing results proven for original DP to f-DP and, as an application, obtain a simple subsampling theorem for f-DP.
                              In addition to the above findings, we introduce a canonical single-parameter family of privacy notions within the f-DP class that is referred to as &#10077;Gaussian differential privacy&#10078; (GDP),
                              defined based on testing two shifted Gaussians.
                              GDP is focal among the f-DP class because of a central limit theorem we prove.
                              More precisely, the privacy guarantees of any hypothesis testing based definition of privacy (including original DP) converges to GDP in the limit under composition.
                              The CLT also yields a computationally inexpensive tool for analyzing the exact composition of private algorithms.
                              Taken together, this collection of attractive properties render f-DP a mathematically coherent, analytically tractable, and versatile framework for private data analysis.
                              Finally, we demonstrate the use of the tools we develop by giving an improved privacy analysis of noisy stochastic gradient descent.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">9:25</td>
                        <td class="slot talk"><a href="#tabs4" data-toggle="collapse" class="accordion-toggle">
                        QUOTIENT: Two-Party Secure Neural Network Training &amp; Prediction
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">
                        Nitin Agrawal, Ali Shahin Shamsabadi, Matthew Kusner and Adria Gascon
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs4">
                                Recently, there has been a wealth of effort devoted to the design of secure protocols for machine learning tasks.
                                Much of this is aimed at enabling secure prediction from highly-accurate Deep Neural Networks (DNNs).
                                However, as DNNs are trained on data, a key question is how such models can be also trained securely.
                                The few prior works on secure DNN training have focused either on designing custom protocols for existing training algorithms
                                or on developing tailored training algorithms and then applying generic secure protocols.
                                In this work, we investigate the advantages of designing training algorithms alongside a novel secure protocol, incorporating optimizations on both fronts.
                                We present QUOTIENT, a new method for discretized training of DNNs,
                                along with a customized secure two-party protocol for it.
                                QUOTIENT incorporates key components of state-of-the-art DNN training such as layer normalization and adaptive gradient methods,
                                and improves upon the state-of-the-art in DNN training in two-party computation.
                                Compared to prior work, we obtain an improvement of 50X in WAN time and 6&#37; in absolute accuracy.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">9:45</td>
                        <td class="break">Coffee break</td>
                    </tr>

                    <tr>
                        <td class="time">10:30</td>
                        <td class="slot talk">
                        Invited talk:
                        <a href="#tabs5" data-toggle="collapse" class="accordion-toggle">
                        Ashwin Machanavajjhala
                        &mdash;
                        Fair Decision Making using Privacy-Protected Data
                        </a> &nbsp;&nbsp;
                        <!-- <a href="https://slideslive.com/38922115" >[View recording part 2]</a> -->
                        <!-- <a href="slides/chaudhuri.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs5">
                              Data collected about individuals is regularly used to make decisions that impact those same individuals. We consider settings where sensitive personal data is used to decide who will receive resources or benefits. While it is well known that there is a tradeoff between protecting privacy and the accuracy of decisions, in this talk, I will describe our recent work on a first-of-its-kind empirical study into the impact of formally private mechanisms (based on differential privacy) on fair and equitable decision-making.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">11:20</td>
                        <td class="slot talk"><a href="#tabs6" data-toggle="collapse" class="accordion-toggle">
                        Spotlight talks
                        </a> &nbsp;&nbsp;
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs6">
                            <ol>
                                <li>[Jonathan Lebensold, William Hamilton, Borja Balle and Doina Precup] Actor Critic with Differentially Private Critic (#08)</li>
                                <li>[Andres Munoz, Umar Syed, Sergei Vassilvitskii and Ellen Vitercik] Private linear programming without constraint violations (#17)</li>
                                <li>[Ios Kotsogiannis, Yuchao Tao, Xi He, Ashwin Machanavajjhala, Michael Hay and Gerome Miklau] PrivateSQL: A Differentially Private SQL Query Engine (#27)</li>
                                <li>[Amrita Roy Chowdhury, Chenghong Wang, Xi He, Ashwin Machanavajjhala and Somesh Jha] Crypt$\epsilon$: Crypto-Assisted Differential Privacy on Untrusted Servers (#31)</li>
                                <li>[Jiaming Xu and Dana Yang] Optimal Query Complexity of Private Sequential Learning (#32)</li>
                                <li>[Hsiang Hsu, Shahab Asoodeh and Flavio Calmon] Discovering Information-Leaking Samples and Features (#43)</li>
                                <li>[Martine De Cock, Rafael Dowsley, Anderson Nascimento, Davis Railsback, Jianwei Shen and Ariel Todoki] Fast Secure Logistic Regression for High Dimensional Gene Data (#44)</li>
                                <li>[Giuseppe Vietri, Grace Tian, Mark Bun, Thomas Steinke and Steven Wu] New Oracle-Efficient Algorithms for Private Synthetic Data Release (#45)</li>
                            </ol>
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">11:30</td>
                        <td class="slot">Poster session</td>
                    </tr>

                    <tr>
                        <td class="time">12:30</td>
                        <td class="break">Lunch break</td>
                    </tr>

                    <tr>
                        <td class="time">14:00</td>
                        <td class="slot talk">
                        Invited talk:
                        <a href="#tabs7" data-toggle="collapse" class="accordion-toggle">
                        Lalitha Sankar
                        &mdash;
			Fair Universal Representations via Generative Models and Model Auditing Guarantees
                        </a> &nbsp;&nbsp;
                        <!-- <a href="slides/smith.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs7">
                              There is a growing demand for ML methods that limit inappropriate use of protected information to avoid both disparate treatment and disparate impact. In this talk, we present Generative Adversarial rePresentations (GAP) as a data-driven framework that leverages recent advancements in adversarial learning to allow a data holder to learn universal representations that decouple a set of sensitive attributes from the rest of the dataset while allowing learning multiple downstream tasks. We will briefly highlight the theoretical and practical results of GAP.
                              <br><br>
                              In the second half of the talk we will focus on model auditing. Privacy concerns have led to the development of privacy-preserving approaches for learning models from sensitive data. Yet, in practice, models (even those learned with privacy guarantees) can inadvertently memorize unique training examples or leak sensitive features. To identify such privacy violations, existing model auditing techniques use finite adversaries defined as machine learning models with (a) access to some finite side information (e.g., a small auditing dataset), and (b) finite capacity (e.g., a fixed neural network architecture). In the second half of the talk, we present requirements under which an unsuccessful attempt to identify privacy violations by a finite adversary implies that no stronger adversary can succeed at such a task. We will do so via parameters that quantify the capabilities of the finite adversary, including the size of the neural network employed by such an adversary and the amount of side information it has access to as well as the regularity of the (perhaps privacy-guaranteeing) audited model.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">14:50</td>
                        <td class="slot talk"><a href="#tabs8" data-toggle="collapse" class="accordion-toggle">
                        Pan-Private Uniformity Testing
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <br/>
                        <span style="font-weight: normal">
                        Kareem Amin, Matthew Joseph and Jieming Mao
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs8">
                              A centrally differentially private algorithm maps raw data to differentially private outputs.
                              In contrast, a locally differentially private algorithm
                              may only access data through public interaction with data holders,
                              and this interaction must be a differentially private function of the data.
                              We study the intermediate model of pan-privacy.
                              Unlike a locally private algorithm, a pan-private algorithm receives data in the clear.
                              Unlike a centrally private algorithm,
                              the algorithm receives data one element at a time
                              and must maintain a differentially private internal state while processing this stream.
                              First, we show that pan-privacy against multiple intrusions on the internal state is
                              equivalent to sequentially interactive local privacy.
                              Next, we contextualize pan-privacy against a single intrusion
                              by analyzing the sample complexity of uniformity testing over domain [k].
                              Focusing on the dependence on k,
                              centrally private uniformity testing has sample complexity &Theta;(&radic;k),
                              while noninteractive locally private uniformity testing has sample complexity &Theta;(k).
                              We show that the sample complexity of pan-private uniformity testing is &Theta;(k<sup>2/3</sup>).
                              By a new &Omega;(k) lower bound for the sequentially interactive setting,
                              we also separate pan-private from sequentially interactive locally private
                              and multi-intrusion pan-private uniformity testing.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">15:10</td>
                        <td class="slot talk"><a href="#tabs9" data-toggle="collapse" class="accordion-toggle">
                        Private Stochastic Convex Optimization: Optimal Rates in Linear Time
                        </a>
                        (contributed talk)
                        &nbsp;&nbsp;
                        <!-- <a href="slides/39.pdf" class="link-paper">[slides]</a> -->
                        <br/>
                        <span style="font-weight: normal">
                        Vitaly Feldman, Tomer Koren and Kunal Talwar
                        </span>
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs9">
                              We study differentially private (DP) algorithms for stochastic convex optimization:
                              the problem of minimizing the population loss given i.i.d. samples from a distribution over convex loss functions.
                              A recent work of Bassily et al. (2019) has established the optimal bound on the excess population loss achievable given n samples.
                              Unfortunately, their algorithm achieving this bound is relatively inefficient:
                              it requires O(min{n<sup>3/2</sup>,n<sup>5/2</sup>/d}) gradient computations, where d is the dimension of the optimization problem.

                              We describe two new techniques for deriving DP convex optimization algorithms both achieving the optimal bound on excess loss and using O(min{n,n<sup>2</sup>/d}) gradient computations.
                              In particular, the algorithms match the running time of the optimal non-private algorithms.
                              The first approach relies on the use of variable batch sizes and is analyzed using the privacy amplification by iteration technique of Feldman et al. (2018).
                              The second approach is based on a general reduction to the problem of localizing an approximately optimal solution with differential privacy.
                              Such localization, in turn, can be achieved using existing (non-private) uniformly stable optimization algorithms.
                              As in the earlier work, our algorithms require a mild smoothness assumption.
                              We also give a linear-time algorithm achieving the optimal bound on the excess loss for the strongly convex case,
                              as well as a faster algorithm for the non-smooth case.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">15:30</td>
                        <td class="break">Coffee break</td>
                    </tr>

                    <tr>
                        <td class="time">16:15</td>
                        <td class="slot talk">
                        Invited talk:
                        <a href="#tabs10" data-toggle="collapse" class="accordion-toggle">
                        Philip Leclerc
                        &mdash;
                        Formal Privacy At Scale: The 2020 Decennial Census TopDown Disclosure Limitation Algorithm
                        </a> &nbsp;&nbsp;
                        <!-- <a href="slides/chaudhuri.pdf" class="link-paper">[slides]</a> -->
                        </td>
                    </tr>
                    <tr>
                        <td colspan="2" class="hiddenRow">
                            <div class="accordion-body collapse talk-abstract" id="tabs10">
                            To control vulnerabilities to reconstruction-abetted re-identification attacks that leverage massive external data stores and cheap computation, the U.S. Census Bureau has elected to adopt a formally private approach to disclosure limitation in the 2020 Decennial Census of Population and Housing. To this end, a team of disclosure limitation specialists have worked over the past 3 years to design and implement the U.S. Census Bureau TopDown Disclosure Limitation Algorithm (TDA). This formally private algorithm generates Persons and Households micro-data, which will then be tabulated to produce the final set of demographic statistics published as a result of the 2020 Census enumeration. In this talk, I outline the main features of TDA, describe the current iteration of the process used to help policy makers decide how to set and allocate privacy-loss budget, and outline known issues with - and intended fixes for - the current implementation of TDA.
                            </div>
                        </td>
                    </tr>

                    <tr>
                        <td class="time">17:05</td>
                        <td class="slot">Panel Discussion</td>
                    </tr>

                    <tr>
                        <td class="time">17:55</td>
                        <td class="slot">Closing</td>
                    </tr>

        		    </tbody>
        		</table>
            </div>
            <div class="row">
                <div  class="col-sm-8 col-sm-offset-2">
                    <h4 id="slideslive">View on SlidesLive</h4>
                    <table class="table schedule">
                        <tbody>
                          <tr>
                              <td class="time">8:10</td>
                              <td class="slot talk">
                                <a href="#description1" data-toggle="collapse" class="accordion-toggle">
                                1st block
                                </a>
                                &#8195;&#8195;&#8195;&nbsp;
                                <a href="https://slideslive.com/38922114" >[View recording]</a>
                              </td>
                          </tr>
                          <tr>
                              <td colspan="2" class="hiddenRow">
                                  <div class="accordion-body collapse talk-abstract" id="description1">
                                  Watch the opening remarks, Brendan McMahan's invited talk, and the first two contributed talks.
                                  </div>
                              </td>
                          </tr>
                          <tr>
                              <td class="time">10:30</td>
                              <td class="slot talk">
                                <a href="#description2" data-toggle="collapse" class="accordion-toggle">
                                2nd block
                                </a>
                                &#8195;&#8195;&#8195;
                                <a href="https://slideslive.com/38922115" >[View recording]</a>
                              </td>
                          </tr>
                          <tr>
                              <td colspan="2" class="hiddenRow">
                                  <div class="accordion-body collapse talk-abstract" id="description2">
                                  Watch Ashwin Machanavajjhala's invited talk.
                                  </div>
                              </td>
                          </tr>
                          <tr>
                              <td class="time">14:00</td>
                              <td class="slot talk">
                                <a href="#description3" data-toggle="collapse" class="accordion-toggle">
                                3rd block
                                </a>
                                &#8195;&#8195;&#8195;&nbsp;
                                <a href="https://slideslive.com/38922116" >[View recording]</a>
                              </td>
                          </tr>
                          <tr>
                              <td colspan="2" class="hiddenRow">
                                  <div class="accordion-body collapse talk-abstract" id="description3">
                                  Watch  Lalitha Sankar's invited talk and two more contributed talks.
                                  </div>
                              </td>
                          </tr>
                          <tr>
                              <td class="time">16:15</td>
                              <td class="slot talk">
                                <a href="#description4" data-toggle="collapse" class="accordion-toggle">
                                4th block
                                </a>
                                &#8195;&#8195;&#8195;&nbsp;
                                <a href="https://slideslive.com/38922117" >[View recording]</a>
                              </td>
                          </tr>
                          <tr>
                              <td colspan="2" class="hiddenRow">
                                  <div class="accordion-body collapse talk-abstract" id="description4">
                                  Watch Philip Leclerc's invited talk and the panel discussion.
                                  </div>
                              </td>
                          </tr>
                        </tbody>
                  </table>
                </div>
            </div>
        </div>
    </section>

    <!-- Accepted Papers -->

    <section id="papers" class="container content-section text-center">
        <div class="row">
        <div class="col-lg-8 col-lg-offset-2">
            <h2>Accepted Papers</h2>
                <!-- <h4 style="color: #d07200;">
                Links to pdfs as well as abstracts will be added soon.
                </h4> -->

                <!-- <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            AUTHOTS
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs8" class="paper-title">
                            TITLE (paper with PDF)
                        </a> &nbsp;&nbsp;
                        <a href="papers/13.pdf" class="link-paper">[PDF]</a>
                    </div>
                    <div id="abs8" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        AUTHORS
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs1" class="paper-title">
                        TITLE (paper with Arvix link)
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1811.11124" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs1" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div>
                </div>

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                            AUTHORS
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs11" class="paper-title">
                            TITLE (paper with contributed talk) <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1806.03287" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs11" class="panel-footer panel-paper-footer collapse">
                    ABSTRACT
                    </div>
                </div> -->

                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Clément Canonne, Gautam Kamath, Audra McMillan, Jonathan Ullman and Lydia Zakynthinou
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs1" class="paper-title">
                        Private Identity Testing for High-Dimensional Distributions
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1905.11947.pdf" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs1" class="panel-footer panel-paper-footer collapse">
                    In this work we present novel differentially private identity (goodness-of-fit) testers for natural and widely studied classes of multivariate product distributions:
                    Gaussians in &#8477;<sup>d</sup> with known covariance and product distributions over {&plusmn;1}<sup>d</sup>.
                    Our testers have improved sample complexity compared to those derived from previous techniques,
                    and are the first testers whose sample complexity matches the order-optimal minimax sample complexity of O(d<sup>1/2</sup>&#47;&alpha;<sup>2</sup>) in many parameter regimes.
                    We construct two types of testers, exhibiting tradeoffs between sample complexity and computational complexity.
                    Finally, we provide a two-way reduction between testing a subclass of multivariate product distributions and testing univariate distributions,
                    and thereby obtain upper and lower bounds for testing this subclass of product distributions.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                          Kwang-Sung Jun and Francesco Orabona
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs2" class="paper-title">
                        Parameter-Free Locally Differentially Private Stochastic Subgradient Descent
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1911.09564" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs2" class="panel-footer panel-paper-footer collapse">
                    We consider the problem of minimizing a convex risk with stochastic subgradients guaranteeing &#x3F5;-locally differentially private (&#x3F5;-LDP).
                    While it has been shown that stochastic optimization is possible with &#x3F5;-LDP via the standard SGD (Song et al., 2013),
                    its convergence rate largely depends on the learning rate, which must be tuned via repeated runs.
                    Further, tuning is detrimental to privacy loss since it significantly increases the number of gradient requests.
                    In this work, we propose BANCO (Betting Algorithm for Noisy COins),
                    the first &#x3F5;-LDP SGD algorithm that essentially matches the convergence rate of the tuned SGD without any learning rate parameter,
                    reducing privacy loss and saving privacy budget.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Seth Neel, Zhiwei Steven Wu, Aaron Roth and Giuseppe Vietri
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs3" class="paper-title">
                        Differentially Private Objective Perturbation: Beyond Smoothness and Convexity
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1909.01783" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs3" class="panel-footer panel-paper-footer collapse">
                    One of the most effective algorithms for differentially private learning and optimization is objective perturbation.
                    This technique augments a given optimization problem (e.g. deriving from an ERM problem) with a random linear term,
                    and then exactly solves it.
                    However, to date, analyses of this approach crucially rely on the convexity and smoothness of the objective function.
                    We give two algorithms that extend this approach substantially.
                    The first algorithm requires nothing except boundedness of the loss function,
                    and operates over a discrete domain.
                    Its privacy and accuracy guarantees hold even without assuming convexity.
                    The second algorithm operates over a continuous domain and requires only that the loss function be bounded and Lipschitz in its continuous parameter.
                    Its privacy analysis does not even require convexity.
                    Its accuracy analysis does require convexity,
                    but does not require second order conditions like smoothness.
                    We complement our theoretical results with an empirical evaluation of the non-convex case,
                    in which we use an integer program solver as our optimization oracle.
                    We find that for the problem of learning linear classifiers,
                    directly optimizing for 0/1 loss using our approach can out-perform the more standard approach of privately optimizing a convex-surrogate loss function on the Adult dataset.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Di Wang, Huanyu Zhang, Marco Gaboardi and Jinhui Xu
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs4" class="paper-title">
                        Estimating Smooth GLM in Non-interactive Local Differential Privacy Model with Public Unlabeled Data
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_5.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs4" class="panel-footer panel-paper-footer collapse">
                    In this paper, we study the problem of estimating smooth Generalized Linear Models (GLM) in the Non-interactive Local Differential Privacy (NLDP) model.
                    Different from its classical setting, our model allows the server to process some additional public but unlabeled data.
                    By using Stein’s lemma and its variants, we first show that there is an
                    (&varepsilon;,&delta;)-NLDP algorithm for GLM (under some mild assumptions),
                    if each data record is i.i.d sampled from some sub-Gaussian distribution with bounded &ell;<sub>1</sub>-norm.
                    Then with high probability, the sample complexity of public and private data,
                    for the algorithm to achieve an α estimation error (in &ell;<sub>&infin;</sub>-norm),
                    is O(p<sup>2</sup>&alpha;<sup>−2</sup>) and O(p<sup>2</sup>&alpha;<sup>−2</sup>&varepsilon;<sup>−2</sup>) respectively
                    if &alpha; is not too small (i.e., &alpha; ≥ &Omega;(1/√p)),
                    where p is the dimensionality of the data.
                    This is a significant improvement over the previously known quasi-polynomial (in &alpha;)
                    or exponential (in p) complexity of GLM with no public data.
                    We demonstrate the effectiveness of our algorithms through experiments on both synthetic and real world datasets.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Liwei Song, Reza Shokri and Prateek Mittal
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs5" class="paper-title">
                        Privacy vs Robustness (against Adversarial Examples) in Machine Learning
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_6.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs5" class="panel-footer panel-paper-footer collapse">
                    Research into challenges of machine learning models typically considers the security domain and the privacy domain separately.
                    It is thus unclear whether the defenses in one domain will have any unexpected impact on the other domain.
                    In this paper, we combine two domains together by investigating the interplay between membership inference and
                    robustness against adversarial examples in machine learning.
                    By performing membership inference attacks against both robust models and natural (undefended) models,
                    we find that the adversarial defense methods, although increase the model robustness against adversarial examples,
                    also make the model more vulnerable to membership inference attacks,
                    indicating a potential conflict between privacy and robustness in machine learning.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Jonathan Lebensold, William Hamilton, Borja Balle and Doina Precup
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs6" class="paper-title">
                        Actor Critic with Differentially Private Critic
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1910.05876" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs6" class="panel-footer panel-paper-footer collapse">
                    Reinforcement learning algorithms are known to be sample inefficient, and often performance on one task can be substantially improved by leveraging information (e.g., via pre-training) on other related tasks.
                    In this work, we propose a technique to achieve such knowledge transfer in cases where agent trajectories contain sensitive or private information, such as in the healthcare domain.
                    Our approach leverages a differentially private policy evaluation algorithm to initialize an actor-critic model and improve the effectiveness of learning in downstream tasks.
                    We empirically show this technique increases sample efficiency in resource-constrained control problems while preserving the privacy of trajectories collected in an upstream task.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Samyadeep Basu, Rauf Izmailov and Chris Mesterharm
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs7" class="paper-title">
                        Membership Model Inversion Attacks for Deep Networks
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1910.04257.pdf" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs7" class="panel-footer panel-paper-footer collapse">
                    With the increasing adoption of AI,
                    inherent security and privacy vulnerabilities for machine learning systems are being discovered.
                    One such vulnerability makes it possible for an adversary to obtain private information about the types of instances used to train the targeted machine learning model.
                    This so-called model inversion attack is based on sequential leveraging of classification scores towards obtaining high confidence representations for various classes.
                    However, for deep networks, such procedures usually lead to unrecognizable representations that are useless for the adversary.
                    In this paper, we introduce a more realistic definition of model inversion,
                    where the adversary is aware of the general purpose of the attacked model
                    (for instance, whether it is an OCR system or a facial recognition system),
                    and the goal is to find realistic class representations within the corresponding lower-dimensional manifold
                    (of, respectively, general symbols or general faces).
                    To that end, we leverage properties of generative adversarial networks for constructing a connected lower-dimensional manifold,
                    and demonstrate the efficiency of our model inversion attack that is carried out within that manifold.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Gautam Kamath, Janardhan Kulkarni, Zhiwei Steven Wu and Huanyu Zhang
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs8" class="paper-title">
                        Privately Learning Markov Random Fields
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_11.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs8" class="panel-footer panel-paper-footer collapse">
                    We consider the problem of learning Markov Random Fields,
                    particularly the special case of the Ising model,
                    under the constraint of differential privacy.
                    This includes both structure learning,
                    where we try to estimate the underlying graph structure of the model,
                    as well as the harder goal of parameter learning,
                    in which we additionally estimate the parameter on each clique.
                    We provide algorithms and lower bounds for both problems under a variety of privacy models.
                    While the non-private sample complexity bounds for these two problems are both logarithmic in the dimension,
                    we show that this is not the case under differential privacy.
                    In particular, we investigate the sample complexity bounds for both problems under the constraints of pure,
                    approximate, and concentrated differential privacy.
                    We show that only structure learning under approximate differential privacy
                    has logarithmic sample complexity in the dimension,
                    while a change in either the learning goal or the privacy notion would necessitate a polynomial dependence.
                    This suggests that if we are operating on very high-dimensional data,
                    the aforementioned setting is the only one which is tractable.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Lovedeep Gondara and Ke Wang
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs9" class="paper-title">
                        Differentially Private Survival Function Estimation
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1910.05108.pdf" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs9" class="panel-footer panel-paper-footer collapse">
                    Survival function estimation is used in many disciplines,
                    but it is most common in medical analytics in the form of the Kaplan-Meier estimator.
                    Sensitive data (patient records) is used in the estimation without any explicit control on the information leakage,
                    which is a significant privacy concern.
                    We propose a first differentially private estimator of the survival function and
                    show that it can be easily extended to provide differentially private confidence intervals and test statistics
                    without spending any extra privacy budget.
                    We further provide extensions for differentially private estimation of the competing risk cumulative incidence function.
                    Using nine real-life clinical datasets,
                    we provide empirical evidence that our proposed method provides good utility while simultaneously providing strong privacy guarantees.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Ang Li, Jiayi Guo, Huanrui Yang and Yiran Chen
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs10" class="paper-title">
                        DeepObfuscator: Adversarial Training Framework for Privacy-Preserving Image Classification
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1909.04126.pdf" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs10" class="panel-footer panel-paper-footer collapse">
                    Deep learning has been widely utilized in many computer vision applications and achieved remarkable commercial success.
                    However, running deep learning models on mobile devices is generally challenging due to limitation of the available computing resources.
                    It is common to let the users send their service requests to cloud servers that run the large-scale deep learning models to process.
                    Sending the data associated with the service requests to the cloud, however, impose risks on the user data privacy.
                    Some prior arts proposed sending the features extracted from raw data (e.g., images) to the cloud.
                    Unfortunately, these extracted features can still be exploited by attackers to recover raw images and to infer embedded private attributes (e.g., age, gender, etc.).
                    In this paper, we propose an adversarial training framework <i>DeepObfuscator</i> that can prevent extracted features from being utilized to reconstruct raw images and infer private attributes,
                    while retaining the useful information for the intended cloud service (i.e., image classification).
                    DeepObfuscator includes a learnable encoder,  namely,  obfuscator  that  is  designed  to  hide  privacy-related sensitive information from the features by performing our proposed adversarial training algorithm.
                    The proposed algorithm is designed by simulating the game between an attacker who makes efforts to reconstruct raw images and infer private attributes from the extracted features
                    and a defender who aims to protect user privacy.
                    Our experiments on CelebA dataset show that the quality of the reconstructed images from the obfuscated features of the raw image is dramatically decreased
                    from 0.9458 to 0.3175 in terms of multi-scale structural similarity (MS-SSIM).
                    The person in the reconstructed image, hence, becomes hardly to be re-identified.
                    The classification accuracy of the inferred private attributes that can be achieved by the attacker drops down to a random-guessing level, e.g.,
                    the accuracy of gender is reduced from 97.36&#37; to 58.85&#37;.
                    As a comparison, the accuracy of the intended classification tasks performed via the cloud service drops by only 2&#37;.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Fatemehsadat Mireshghallah, Mohammadkazem Taram, Prakash Ramrakhyani, Dean Tullsen and Hadi Esmaeilzadeh
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs11" class="paper-title">
                        Shredder: Learning Noise Distributions to Protect Inference Privacy
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1905.11814" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs11" class="panel-footer panel-paper-footer collapse">
                    Sheer amount of computation in deep neural networks has pushed their execution to the cloud.
                    This de facto cloud-hosted inference, however, raises serious privacy concerns as private data is communicated and stored in remote servers.
                    The data could be mishandled by cloud providers, used for unsolicited analytics, or simply compromised through network and system security vulnerability.
                    To that end, this paper devises SHREDDER that reduces the information content of the communicated data without diminishing the cloud's ability to maintain acceptably high accuracy.
                    To that end, SHREDDER learns two sets of noise distributions whose samples, named multiplicative and additive noise tensors, are applied to the communicated data while maintaining the inference accuracy.
                    The key idea is that SHREDDER learns these noise distributions offline without altering the topology or the weights of the pre-trained network.
                    SHREDDER repeatedly learns sample noise tensors from the distributions by casting the tensors as a set of trainable parameters while keeping the weights constant.
                    Since the key idea is learning the noise, we are able to devise a loss function that strikes a balance between accuracy and information degradation.
                    To this end, we use self-supervision to train the noise tensors to achieve an intermediate representation of the data that contains less private information.
                    Experimentation with real-world deep neural networks shows that, compared to the original execution, SHREDDER reduces the mutual information between the input and the communicated data by 66.90&#37;,
                    and yields a misclassification rate of 94.5&#37; over private labels, significantly reducing adversary's ability to infer private data,
                    while sacrificing only 1.74&#37; loss in accuracy without any knowledge about the private labels.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Jinshuo Dong, Aaron Roth and Weijie Su
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs12" class="paper-title">
                        Gaussian Differential Privacy <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1905.02383" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs12" class="panel-footer panel-paper-footer collapse">
                      Differential privacy has seen remarkable success as a rigorous and practical formalization of data privacy in the past decade.
                      This privacy definition and its divergence based relaxations, however, have several acknowledged weaknesses, either in handling composition of private algorithms or in analyzing important primitives like privacy amplification by subsampling.
                      Inspired by the hypothesis testing formulation of privacy, this paper proposes a new relaxation, which we term &#10077;f-differential privacy&#10078; (f-DP).
                      This notion of privacy has a number of appealing properties and, in particular, avoids difficulties associated with divergence based relaxations.
                      First, f-DP preserves the hypothesis testing interpretation.
                      In addition, f-DP allows for lossless reasoning about composition in an algebraic fashion.
                      Moreover, we provide a powerful technique to import existing results proven for original DP to f-DP and, as an application, obtain a simple subsampling theorem for f-DP.
                      In addition to the above findings, we introduce a canonical single-parameter family of privacy notions within the f-DP class that is referred to as &#10077;Gaussian differential privacy&#10078; (GDP),
                      defined based on testing two shifted Gaussians.
                      GDP is focal among the f-DP class because of a central limit theorem we prove.
                      More precisely, the privacy guarantees of any hypothesis testing based definition of privacy (including original DP) converges to GDP in the limit under composition.
                      The CLT also yields a computationally inexpensive tool for analyzing the exact composition of private algorithms.
                      Taken together, this collection of attractive properties render f-DP a mathematically coherent, analytically tractable, and versatile framework for private data analysis.
                      Finally, we demonstrate the use of the tools we develop by giving an improved privacy analysis of noisy stochastic gradient descent.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Andres Munoz, Umar Syed, Sergei Vassilvitskii and Ellen Vitercik
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs13" class="paper-title">
                        Private Linear Programming Without Constraint Violations
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_17.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs13" class="panel-footer panel-paper-footer collapse">
                    We show how to solve linear programs whose constraints
                    depend on private data. Existing techniques allow constraint
                    violations whose magnitude is bounded interms of the
                    differential privacy parameters &varepsilon; and &delta;.
                    In many applications, however, the constraints cannot be violated under any circumstances.
                    We demonstrate that straightforward applications of common differential privacy tools,
                    such as Laplace noise and the exponential mechanism,
                    are inadequate for guaranteeing that the constraints are satisfied.
                    We then present a new differentially private mechanism that takes as input a linear program
                    and releases a solution that satisfies the constraints and differs from the optimal solution
                    by only a small amount.
                    Empirically, we show that alternative mechanisms do violate constraints in practice.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Hafiz Imtiaz, Jafar Mohammadi and Anand D. Sarwate
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs14" class="paper-title">
                        Correlation-Assisted Distributed Differentially Private Estimation
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1904.10059" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs14" class="panel-footer panel-paper-footer collapse">
                    Many applications of machine learning, such as human health research, involve processing private or sensitive information.
                    Privacy concerns may impose significant hurdles to collaboration in scenarios where there are multiple sites holding data and the goal is to estimate properties jointly across all datasets.
                    Differentially private decentralized algorithms can provide strong privacy guarantees.
                    However, the accuracy of the joint estimates may be poor when the datasets at each site are small.
                    This paper proposes a new framework, Correlation Assisted Private Estimation (CAPE), for designing privacy-preserving decentralized algorithms with better accuracy guarantees in an honest-but-curious model.
                    CAPE can be used in conjunction with the functional mechanism for statistical and machine learning optimization problems.
                    A tighter characterization of the functional mechanism is provided that allows CAPE to achieve the same performance as a centralized algorithm in the decentralized setting using all datasets.
                    Empirical results on regression and neural network problems for both synthetic and real datasets show that differentially private methods can be competitive with non-private algorithms in many scenarios of interest.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Naoise Holohan, Stefano Braghin, Pol Mac Aonghusa and Killian Levacher
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs15" class="paper-title">
                        Diffprivlib: The IBM Differential Privacy Library
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1907.02444" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs15" class="panel-footer panel-paper-footer collapse">
                      Since its conception in 2006, differential privacy has emerged as the de-facto standard in data privacy, owing to its robust mathematical guarantees, generalised applicability and rich body of literature.
                      Over the years, researchers have studied differential privacy and its applicability to an ever-widening field of topics.
                      Mechanisms have been created to optimise the process of achieving differential privacy, for various data types and scenarios.
                      Until this work however, all previous work on differential privacy has been conducted on a ad-hoc basis, without a single, unifying codebase to implement results.
                      In this work, we present the IBM Differential Privacy Library, a general purpose, open source library for investigating, experimenting and developing differential privacy applications in the Python programming language.
                      The library includes a host of mechanisms, the building blocks of differential privacy, alongside a number of applications to machine learning and other data analytics tasks.
                      Simplicity and accessibility has been prioritised in developing the library, making it suitable to a wide audience of users, from those using the library for their first investigations in data privacy, to the privacy experts looking to contribute their own models and mechanisms for others to use.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Antti Koskela, Joonas Jälkö and Antti Honkela
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs16" class="paper-title">
                        Computing Exact Guarantees for Differential Privacy
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1906.03049" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs16" class="panel-footer panel-paper-footer collapse">
                    Differentially private (DP) machine learning has recently become popular.
                    The privacy loss of DP algorithms is commonly reported using (&epsilon;,&delta;)-DP.
                    In this paper, we propose a numerical accountant for evaluating the privacy loss for algorithms with continuous one dimensional output.
                    This accountant can be applied to the subsampled multidimensional Gaussian mechanism which underlies the popular DP stochastic gradient descent.
                    The proposed method is based on a numerical approximation of an integral formula which gives the exact (&epsilon;,&delta;)-values.
                    The approximation is carried out by discretising the integral and by evaluating discrete convolutions using the fast Fourier transform algorithm.
                    We give both theoretical error bounds and numerical error estimates for the approximation.
                    Experimental comparisons with state-of-the-art techniques demonstrate significant improvements in bound tightness and/or computation time.
                    Python code for the method can be found in Github.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Joonas Jälkö, Antti Honkela and Samuel Kaski
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs17" class="paper-title">
                        Privacy-Preserving Data Sharing via Probabilistic Modelling
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_21.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs17" class="panel-footer panel-paper-footer collapse">
                    Differential privacy allows quantifying privacy loss from computations using sensitive personal data.
                    This loss grows with the number of accesses to the data,
                    making it hard to open the use of such data while respecting privacy.
                    Instead of accessing the data multiple times,
                    we propose a method of fitting a probabilistic model on the data using privacy preserving modelling techniques.
                    From this probabilistic model we sample a new synthetic dataset that may be subjected to unlimited amount of future analysis,
                    without affecting the privacy guarantees.
                    We demonstrate empirically that similar statistical discoveries can be made from the synthetic as the original data.
                    We expect the method to have broad use in sharing anonymized versions of key data sets for research.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Nitin Agrawal, Ali Shahin Shamsabadi, Matthew Kusner and Adria Gascon
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs18" class="paper-title">
                        QUOTIENT: Two-Party Secure Neural Network Training and Prediction
                        <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/pdf/1907.03372.pdf" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs18" class="panel-footer panel-paper-footer collapse">
                      Recently, there has been a wealth of effort devoted to the design of secure protocols for machine learning tasks.
                      Much of this is aimed at enabling secure prediction from highly-accurate Deep Neural Networks (DNNs).
                      However, as DNNs are trained on data, a key question is how such models can be also trained securely.
                      The few prior works on secure DNN training have focused either on designing custom protocols for existing training algorithms
                      or on developing tailored training algorithms and then applying generic secure protocols.
                      In this work, we investigate the advantages of designing training algorithms alongside a novel secure protocol, incorporating optimizations on both fronts.
                      We present QUOTIENT, a new method for discretized training of DNNs,
                      along with a customized secure two-party protocol for it.
                      QUOTIENT incorporates key components of state-of-the-art DNN training such as layer normalization and adaptive gradient methods,
                      and improves upon the state-of-the-art in DNN training in two-party computation.
                      Compared to prior work, we obtain an improvement of 50X in WAN time and 6&#37; in absolute accuracy.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Dingfan Chen, Ning Yu, Yang Zhang and Mario Fritz
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs19" class="paper-title">
                        GAN-Leaks: A Taxonomy of Membership Inference Attacks against GANs
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_23.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs19" class="panel-footer panel-paper-footer collapse">
                    In recent years, deep learning has achieved overwhelming success,
                    spanning from discriminative models to generative models.
                    In particular, generative adversarial networks (GANs) have facilitated
                    a new level of performance in a myriad of areas,
                    ranging from media manipulation to sanitized dataset generation.
                    Despite the great success, the potential risks of privacy breach caused by GANs
                    have not been analyzed systematically.
                    In this paper, we focus on membership inference attack against GANs
                    that reveals information about the training data used for victim models.
                    Specifically,  we present the first taxonomy of membership inference attacks,
                    comprising not only existing attacks but also our novel ones.
                    In addition, we propose the first generic attack model that can be instantiated
                    in a large range of settings according to the adversary’s knowledge about the victim models.
                    We complement the systematic analysis of attack performance by a comprehensive experimental  study,
                    that  investigates  the  effectiveness  of  various  attacks  w.r.t. model type and training configurations,
                    over three diverse application scenarios (i.e., images, medical data, and location data).
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Si Kai Lee, Luigi Gresele, Mijung Park and Krikamol Muandet
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs20" class="paper-title">
                        Private Causal Inference using Propensity Scores
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1905.12592" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs20" class="panel-footer panel-paper-footer collapse">
                    The use of inverse probability weighting (IPW) methods to
                    estimate the causal effect of treatments from observational
                    studies is widespread in econometrics, medicine and social
                    sciences. Although these studies often involve sensitive
                    information, thus far there has been no work on
                    privacy-preserving IPW methods. We address this by
                    providing a novel framework for privacy-preserving IPW
                    (PP-IPW) methods. We include a theoretical analysis of the
                    effects of our proposed privatisation procedure on the
                    estimated average treatment effect, and evaluate our PP-IPW
                    framework on synthetic, semi-synthetic and real datasets.
                    The empirical results are consistent with our theoretical
                    findings.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Kareem Amin, Matthew Joseph and Jieming Mao
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs21" class="paper-title">
                        Pan-Private Uniformity Testing <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1911.01452" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs21" class="panel-footer panel-paper-footer collapse">
                      A centrally differentially private algorithm maps raw data to differentially private outputs.
                      In contrast, a locally differentially private algorithm
                      may only access data through public interaction with data holders,
                      and this interaction must be a differentially private function of the data.
                      We study the intermediate model of pan-privacy.
                      Unlike a locally private algorithm, a pan-private algorithm receives data in the clear.
                      Unlike a centrally private algorithm,
                      the algorithm receives data one element at a time
                      and must maintain a differentially private internal state while processing this stream.
                      First, we show that pan-privacy against multiple intrusions on the internal state is
                      equivalent to sequentially interactive local privacy.
                      Next, we contextualize pan-privacy against a single intrusion
                      by analyzing the sample complexity of uniformity testing over domain [k].
                      Focusing on the dependence on k,
                      centrally private uniformity testing has sample complexity &Theta;(&radic;k),
                      while noninteractive locally private uniformity testing has sample complexity &Theta;(k).
                      We show that the sample complexity of pan-private uniformity testing is &Theta;(k<sup>2/3</sup>).
                      By a new &Omega;(k) lower bound for the sequentially interactive setting,
                      we also separate pan-private from sequentially interactive locally private
                      and multi-intrusion pan-private uniformity testing.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Ios Kotsogiannis, Yuchao Tao, Xi He, Ashwin Machanavajjhala, Michael Hay and Gerome Miklau
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs22" class="paper-title">
                        PrivateSQL: A Differentially Private SQL Query Engine
                        </a> &nbsp;&nbsp;
                        <a href="http://www.vldb.org/pvldb/vol12/p1371-kotsogiannis.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs22" class="panel-footer panel-paper-footer collapse">
                    Differential privacy is considered a de facto standard for private data analysis.
                    However, the definition and much of the supporting literature applies to flat tables.
                    While there exist variants of the definition and specialized algorithms for specific types of relational data (e.g. graphs),
                    there isn’t a general privacy definition for multi-relational schemas with constraints,
                    and no system that permits accurate differentially private answering of SQL queries
                    while imposing a fixed privacy budget across all queries posed by the analyst.
                    This work presents PrivateSQL, a first-of-its-kind end-to-end differentially private relational database system.
                    PrivateSQL allows an analyst to query data stored in a standard database management system
                    using a rich class of SQL counting queries.
                    PrivateSQL adopts a novel generalization of differential privacy to multi-relational data
                    that takes into account constraints in the schema like foreign keys,
                    and allows the data owner to flexibly specify entities in the schema that need privacy.
                    PrivateSQL ensures a fixed privacy loss across all the queries posed by the analyst
                    by answering queries on private synopses generated from several views over the base relation
                    that are tuned to have low error on a representative query workload.
                    We experimentally evaluate PrivateSQL on a real-world dataset and a work-load of more than 3,600 queries.
                    We show that for 50&#37; of the queries PrivateSQL offers at least 1,000x better error rates
                    than solutions adapted from prior work.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Chao Jin, Ahmad Qaisar Ahmad Al Badawi, Balagopal Unnikrishnan, Jie Lin, Fook Mun Chan, James Brown, J. Peter Campbell, Michael F. Chiang, Jayashree Kalpathy-Cramer, Vijay Chandrasekhar, Pavitra Krishnaswamy and Khin Mi Mi Aung
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs23" class="paper-title">
                        CareNets: Efficient Homomorphic CNN for High Resolution Images
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_28.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs23" class="panel-footer panel-paper-footer collapse">
                    Deep learning as a service paradigms are increasingly
                    employed for image-based applications spanning surveillance,
                    healthcare, biometrics, and e-commerce. Typically, trained
                    convolutional neural networks (CNNs) are hosted on cloud
                    infrastructure, and applied for inference on input images.
                    There is interest in approaches to enhance data privacy and
                    security in such settings. Fully homomorphic encryption (FHE)
                    can address this need as it caters to computations on
                    encrypted data, but poses intensive computational burden.
                    Prior works have proposed approaches to alleviate this burden
                    for 32×32 images, but practical applications require at least
                    10X higher resolution. Here, we present CareNets: Compact
                    and Resource Efficient CNN for homomorphic inference on
                    encrypted high-resolution images. Our approach is based on a
                    novel compact packing scheme that packs CNN inputs, weights
                    and activations densely into HE ciphertexts; and integrates
                    them into the CNN computation flow. We implement CareNets
                    using a GPU-accelerated FHE library for CNN inference on
                    encrypted retinal images of size 96×96 and 256×256. Our
                    results show that CareNets achieves over 32.78X speedup,
                    45X improvement in memory efficiency, and 5851X reduction
                    in transferred message size while maintaining accuracy within
                    3% of the non-encrypted CNN baselines.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Alexandra Schofield, Gregory Yauney and David Mimno
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs24" class="paper-title">
                        Combatting The Challenges of Local Privacy for Distributional Semantics with Compression
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_29.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs24" class="panel-footer panel-paper-footer collapse">
                    Traditional methods for adding locally private noise to bag-of-words features
                    overwhelm the true signal in the text data, removing the properties of sparsity
                    and non-negativity often relied upon by distributional semantic models. We
                    argue the formulation of limited-precision local privacy, which guarantees
                    privacy between documents of less than a user-specified maximum distance, is a
                    more appropriate framework for bag-of-words features. To reduce the number of
                    features to which we must add random noise, we also compress word features before
                    adding noise, then decompress those features before model inference. We test
                    randomized methods of aggregation as well as methods informed by distributional
                    properties of words. Applying LDA and LSA to synthetic and real data, we show that
                    these approaches produce distributional models closer to those in the original data.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Vitaly Feldman, Tomer Koren and Kunal Talwar
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs25" class="paper-title">
                        Private Stochastic Convex Optimization: Optimal Rates in Linear Time
                        <font color="#d07200"><b>(contributed talk)</b></font>
                        </a> &nbsp;&nbsp;
                        <a href="http://vtaly.net/papers/FKT_fast_DPSCO.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs25" class="panel-footer panel-paper-footer collapse">
                    We study differentially private (DP) algorithms for stochastic convex optimization:
                    the problem of minimizing the population loss given i.i.d. samples from a distribution over convex loss functions.
                    A recent work of Bassily et al. (2019) has established the optimal bound on the excess population loss achievable given n samples.
                    Unfortunately, their algorithm achieving this bound is relatively inefficient:
                    it requires O(min{n<sup>3/2</sup>,n<sup>5/2</sup>/d}) gradient computations, where d is the dimension of the optimization problem.

                    We describe two new techniques for deriving DP convex optimization algorithms both achieving the optimal bound on excess loss and using O(min{n,n<sup>2</sup>/d}) gradient computations.
                    In particular, the algorithms match the running time of the optimal non-private algorithms.
                    The first approach relies on the use of variable batch sizes and is analyzed using the privacy amplification by iteration technique of Feldman et al. (2018).
                    The second approach is based on a general reduction to the problem of localizing an approximately optimal solution with differential privacy.
                    Such localization, in turn, can be achieved using existing (non-private) uniformly stable optimization algorithms.
                    As in the earlier work, our algorithms require a mild smoothness assumption.
                    We also give a linear-time algorithm achieving the optimal bound on the excess loss for the strongly convex case,
                    as well as a faster algorithm for the non-smooth case.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Amrita Roy Chowdhury, Chenghong Wang, Xi He, Ashwin Machanavajjhala and Somesh Jha
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs26" class="paper-title">
                        Crypt&varepsilon;: Crypto-Assisted Differential Privacy on Untrusted Servers
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_31.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs26" class="panel-footer panel-paper-footer collapse">
                    In this work, we propose, Crypt&varepsilon;, a system and programming framework that
                    (1) achieves the accuracy guarantees and algorithmic expressibility of
                    the central model (2) without any trusted data collector like in the
                    local model. Crypt&varepsilon; achieves the “best of both worlds” by employing two
                    non-colluding untrusted servers that run DP programs on encrypted data
                    from the data owners. Although straightforward implementations of DP
                    programs using secure computation tools can achieve the above goal
                    theoretically, in practice they are beset with many challenges such as
                    poor performance and tricky security proofs. To this end, Crypt&varepsilon; allows data
                    analysts to author logical DP programs that are automatically translated to
                    secure protocols that work on encrypted data. These protocols ensure that
                    the untrusted servers learn nothing more than the noisy outputs, thereby
                    guaranteeing &varepsilon;-DP for all Crypt&varepsilon; programs. Crypt&varepsilon; supports a rich class of DP
                    programs that can be expressed via a small set of transformation and
                    measurement operators followed by arbitrary post-processing.  Further, we
                    propose performance optimizations leveraging the fact that the output is
                    noisy. We demonstrate Crypt&varepsilon;’s feasibility for practical DP analysis with
                    extensive empirical evaluations on real datasets.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Jiaming Xu and Dana Yang
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs27" class="paper-title">
                        Optimal Query Complexity of Private Sequential Learning
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1909.09836" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs27" class="panel-footer panel-paper-footer collapse">
                      Motivated by privacy concerns in many practical applications such as Federated Learning,
                      we study a stylized private sequential learning problem:
                      a learner tries to estimate an unknown scalar value,
                      by sequentially querying an external database and receiving binary responses;
                      meanwhile, a third-party adversary observes the learner's queries but not the responses.
                      The learner's goal is to design a querying strategy with the minimum number of queries
                      (optimal query complexity) so that she can accurately estimate the true value,
                      while the adversary even with the complete knowledge of her querying strategy cannot.
                      Prior work has obtained both upper and lower bounds on the optimal query complexity,
                      however, these upper and lower bounds have a large gap in general.
                      In this paper, we construct new querying strategies and prove almost matching upper and lower bounds,
                      providing a complete characterization of the optimal query complexity
                      as a function of the estimation accuracy and the desired levels of privacy.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Benjamin Spector, Andrew Tomkins and Ravi Kumar
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs28" class="paper-title">
                        Preventing Adversarial Use of Datasets through Fair Core-set Construction
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1910.10871" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs28" class="panel-footer panel-paper-footer collapse">
                    We propose improving the privacy properties of a dataset by publishing only a strategically chosen "core-set" of the data containing a subset of the instances.
                    The core-set allows strong performance on primary tasks, but forces poor performance on unwanted tasks.
                    We give methods for both linear models and neural networks and demonstrate their efficacy on data.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Nhathai Phan, My Thai, Devu Shila and Ruoming Jin
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs29" class="paper-title">
                        Differentially Private Lifelong Learning
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_34.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs29" class="panel-footer panel-paper-footer collapse">
                    In this paper, we aim to develop a novel mechanism to preserve
                    differential privacy (DP) in lifelong learning (L2M) for
                    deep neural networks.  Our key idea is to employ functional
                    perturbation approaches in an original algorithm to preserve
                    DP in both learning new tasks and memorizing acquired tasks
                    in the past. Theoretical analysis shows that our mechanism
                    significantly tighten the privacy loss, by avoiding the
                    privacy budget accumulated in the continual learning and
                    memorizing processes.  Thorough evaluations show the
                    effectiveness of our mechanism in preserving DP in L2M. Our
                    study opens a new research avenue by uncovering the trade-off
                    among privacy loss, model utility, and computational
                    efficiency in L2M.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Alessandro Epasto, Hossein Esfandiari, Vahab Mirrokni, Andreas Munoz Medina, Umar Syed and Sergei Vassilvitskii
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs30" class="paper-title">
                        Anonymizing List Data
                        </a> &nbsp;&nbsp;
                        <a href="#" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs30" class="panel-footer panel-paper-footer collapse">
                    We provide novel approximation algorithms for anonymizing list data.
                    List data is ubiquitous in ML applications,
                    for instance in graph adjacency lists or in sparse matrix representations.
                    We introduce a novel notion of anonymity in list datasets,
                    and present polynomial time approximation algorithm for anonymizing such datasets.
                    We show how this notion anonymity allows us to improve over current approximation results for k-anonymization.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Mimansa Jaiswal and Emily Mower Provost
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs31" class="paper-title">
                        Privacy Enhanced Multimodal Neural Representations for Emotion Recognition
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1910.13212" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs31" class="panel-footer panel-paper-footer collapse">
                    Many mobile applications and virtual conversational agents now aim to recognize and adapt to emotions.
                    To enable this, data are transmitted from users' devices and stored on central servers.
                    Yet, these data contain sensitive information that could be used by mobile applications without user's consent or, maliciously, by an eavesdropping adversary.
                    In this work, we show how multimodal representations trained for a primary task, here emotion recognition, can unintentionally leak demographic information, which could override a selected opt-out option by the user.
                    We analyze how this leakage differs in representations obtained from textual, acoustic, and multimodal data.
                    We use an adversarial learning paradigm to unlearn the private information present in a representation and investigate the effect of varying the strength of the adversarial component on the primary task and on the privacy metric, defined here as the inability of an attacker to predict specific demographic information.
                    We evaluate this paradigm on multiple datasets and show that we can improve the privacy metric while not significantly impacting the performance on the primary task.
                    To the best of our knowledge, this is the first work to analyze how the privacy metric differs across modalities and how multiple privacy concerns can be tackled while still maintaining performance on emotion recognition.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Mrinank Sharma, Michael Hutchinson, Siddharth Swaroop, Antti Honkela and Richard Turner
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs32" class="paper-title">
                        Differentially Private Federated Variational Inference
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1911.10563" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs32" class="panel-footer panel-paper-footer collapse">
                    In many real-world applications of machine learning, data are distributed across many clients and cannot leave the devices they are stored on.
                    Furthermore, each client's data, computational resources and communication constraints may be very different.
                    This setting is known as federated learning, in which privacy is a key concern.
                    Differential privacy is commonly used to provide mathematical privacy guarantees.
                    This work, to the best of our knowledge, is the first to consider federated, differentially private, Bayesian learning.
                    We build on Partitioned Variational Inference (PVI) which was recently developed to support approximate Bayesian inference in the federated setting.
                    We modify the client-side optimisation of PVI to provide an (&varepsilon;, &delta;)-DP guarantee.
                    We show that it is possible to learn moderately private logistic regression models in the federated setting that achieve similar performance to models trained non-privately on centralised data.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Hassan Takabi, Robert Podschwadt, Jeff Druce, Curt Wu and Kevin Procopio
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs33" class="paper-title">
                        Privacy preserving Neural Network Inference on Encrypted Data with GPUs
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1911.11377" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs33" class="panel-footer panel-paper-footer collapse">
                      Machine Learning as a Service (MLaaS) has become a growing trend in recent years and several such services are currently offered.
                      MLaaS is essentially a set of services that provides machine learning tools and capabilities as part of cloud computing services.
                      In these settings, the cloud has pre-trained models that are deployed and large computing capacity whereas the clients can use these models to make predictions without having to worry about maintaining the models and the service.
                      However, the main concern with MLaaS is the privacy of the client's data.
                      Although there have been several proposed approaches in the literature to run machine learning models on encrypted data, the performance is still far from being satisfactory for practical use.
                      In this paper, we aim to accelerate the performance of running machine learning on encrypted data using combination of Fully Homomorphic Encryption (FHE), Convolutional Neural Networks (CNNs) and Graphics Processing Units (GPUs).
                      We use a number of optimization techniques, and efficient GPU-based implementation to achieve high performance.
                      We evaluate a CNN whose architecture is similar to AlexNet to classify homomorphically encrypted samples from the Cars Overhead With Context (COWC) dataset.
                      To the best of our knowledge, it is the first time such a complex network and large dataset is evaluated on encrypted data.
                      Our approach achieved reasonable classification accuracy of 95% for the COWC dataset.
                      In terms of performance, our results show that we could achieve several thousands times speed up when we implement GPU-accelerated FHE operations on encrypted floating point numbers.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Casey Meehan and Kamalika Chaudhuri
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs34" class="paper-title">
                        Location Trace Privacy Under Conditional Priors
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1912.04228" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs34" class="panel-footer panel-paper-footer collapse">
                    Providing meaningful privacy to users of location based services is particularly challenging when
                    multiple locations are revealed in a short period of time. This is primarily due to the tremendous
                    degree of dependence that can be anticipated between points. We propose a Rényi differentially private
                    framework for bounding expected privacy loss for conditionally dependent data. Additionally,
                    we demonstrate an algorithm for achieving this privacy under Gaussian process conditional priors.
                    This framework both exemplifies why conditionally dependent data is so challenging to protect
                    and offers a strategy for preserving privacy to within a fixed radius for every user location in a trace.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Zhengli Zhao, Nicolas Papernot, Sameer Singh, Neoklis Polyzotis and Augustus Odena
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs35" class="paper-title">
                        Improving Differentially Private Models via Active Learning
                        </a> &nbsp;&nbsp;
                        <a href="https://arxiv.org/abs/1910.01177" class="link-paper">[arxiv]</a>
                    </div>
                    <div id="abs35" class="panel-footer panel-paper-footer collapse">
                    Broad adoption of machine learning techniques has increased privacy concerns for models trained on sensitive data such as medical records.
                    Existing techniques for training differentially private (DP) models give rigorous privacy guarantees,
                    but applying these techniques to neural networks can severely degrade model performance.
                    This performance reduction is an obstacle to deploying private models in the real world.
                    In this work, we improve the performance of DP models by fine-tuning them through active learning on public data.
                    We introduce two new techniques - DIVERSEPUBLIC and NEARPRIVATE - for doing this fine-tuning in a privacy-aware way.
                    For the MNIST and SVHN datasets, these techniques improve state-of-the-art accuracy for DP models while retaining privacy guarantees.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Hsiang Hsu, Shahab Asoodeh and Flavio Calmon
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs36" class="paper-title">
                        Discovering Information-Leaking Samples and Features
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_43.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs36" class="panel-footer panel-paper-footer collapse">
                    Discovering samples or features which leak information about
                    correlated private data is a key challenge in designing
                    context-aware privacy mechanisms. In this paper, we propose
                    a framework to discover information-leaking samples and
                    features based on an information-theoretic quantity known as
                    the information density. We provide an estimator, TIDE, to
                    approximate the thresholded information density with a
                    provable sample complexity. Our framework is then validated
                    on two real-world datasets providing evidence that the TIDE
                    can potentially be used as a building block to design privacy
                    mechanisms targeting only those information-leaking samples and features.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Martine De Cock, Rafael Dowsley, Anderson Nascimento, Davis Railsback, Jianwei Shen and Ariel Todoki
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs37" class="paper-title">
                        Fast Secure Logistic Regression for High Dimensional Gene Data
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_44.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs37" class="panel-footer panel-paper-footer collapse">
                    When collaboratively training Machine Learning (ML) models
                    with Secure Multiparty Computation (SMC), the price paid for
                    keeping the data of the parties private, is an increase in
                    computational cost and runtime. A careful choice of ML
                    techniques, algorithmic and implementation optimizations are
                    a necessity to enable practical secure ML over distributed
                    datasets. Such optimizations can be tailored to the kind of
                    data and ML problem at hand. To the best of our knowledge,
                    we present the fastest existing SMC implementation for
                    training logistic regression models on high dimensional data.
                    For our largest dataset, we train a model that requires over
                    7 billion secure multiplications; the training completes in
                    about 2 hours in a local area network.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Giuseppe Vietri, Grace Tian, Mark Bun, Thomas Steinke and Steven Wu
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs38" class="paper-title">
                        New Oracle-Efficient Algorithms for Private Synthetic Data Release
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_45.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs38" class="panel-footer panel-paper-footer collapse">
                    We present three new algorithms for constructing differentially
                    private synthetic data—a sanitized version of a sensitive dataset
                    that approximately preserves the answers to a large collection of
                    statistical queries. All three algorithms are oracle-efficient in
                    the sense that they are computationally efficient given access to
                    an optimization oracle, which can implemented using many existing
                    (non-private) sophisticated optimization tools such as integer
                    program solvers. While the accuracy of the synthetic data is
                    contingent on the oracle’s optimization performance, the algorithms
                    satisfy differential privacy even in the worst case.
                    For all three algorithms, we provide theoretical results as well
                    as preliminary empirical evaluation, which shows that the
                    algorithms can efficiently and accurately answer a large collection
                    of queries on the Adult dataset.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Abraham Flaxman
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs39" class="paper-title">
                        Empirical quantification of privacy loss with examples relevant to the 2020 US Census
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_46.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs39" class="panel-footer panel-paper-footer collapse">
                    The 2020 US Census will use differential privacy for disclosure
                    avoidance, employing a new algorithm called TopDown to guarantee
                    privacy loss of at most &varepsilon;. However, it is possible
                    that there is some slack in the bound
                    (which is proven using the sequential composition theorem),
                    and in practice, the privacy loss will be substantially less than &varepsilon;.
                    In this paper, I develop an empirical measure of privacy loss,
                    and apply it to three example algorithms, inspired by some aspects of TopDown,
                    to better understand how the empirical privacy loss might compare
                    to the theoretical guarantee.
                    My results suggest that
                    (1) it is possible to quantify privacy loss empirically in a reasonable amount of time,
                    at least for counting algorithms like TopDown; and
                    (2) it is likely that the empirical privacy loss of
                    hierarchical counting algorithms like TopDown is substantially lower
                    than the privacy bound derived from the serial composition theorem.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Shadi Rahimian, Tribhuvanesh Orekondy and Mario Fritz
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs40" class="paper-title">
                        Differential Privacy Defenses and Sampling Attacks for Membership Inference
                        </a> &nbsp;&nbsp;
                        <a href="papers/PriML2019_paper_47.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs40" class="panel-footer panel-paper-footer collapse">
                    Machine learning models are commonly trained on sensitive
                    and personal data such as pictures, medical records,
                    financial records, etc. A serious breach of the privacy of
                    this training set occurs when an adversary is able to decide
                    whether or not a specific data point in her possession was
                    used to train a model. We protect the model by adding noise
                    to the gradients and as an alternative method, add noise
                    only to the logits to protect the output of the model and
                    evaluate the effect of these two differentially-private
                    techniques against membership inference attacks on 5 diverse
                    datasets. While all previous membership inference attacks
                    rely on access to the posterior probabilities, we present
                    the first attack which only relies on the predicted class
                    label - yet shows high success rate.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Julius Adebayo, Hal Abelson and Danny Weitzner
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs41" class="paper-title">
                        Tensions Between Differential Privacy and Local Explanations for Deep Neural Networks
                        </a> &nbsp;&nbsp;
                        <a href="https://www.dropbox.com/s/exeul9eprt3as5m/dp_explanations_neurips_workshop.pdf?dl=0" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs41" class="panel-footer panel-paper-footer collapse">
                    As deep learning models are beginning to be applied in high stakes and consequential settings,
                    there has been increased interest in interpretability methods for gaining insight into these models.
                    Similarly, privacy is also of paramount importance in these settings.
                    Here, we examine tradeoffs that occur between feature attribution maps,
                    an increasingly popular medium to provide interpretations, and model privacy.
                    Attribution maps reveal information about the features of the input that have the most relevance to the output of a model.
                    On the other hand, differential privacy (DP) seeks to protect against learning information about individual inputs.
                    Consequently, both requirements seem to be at odds.
                    More distressingly, Shokri et al.[1] recently showed that one can infer training set membership using attribution maps.
                    Towards this end, we consider attribution maps derived from models trained to be differentially private.
                    Empirically, we find that there is a trade-off between privacy and interpretability.
                    Attribution maps that seek to reconstruct the input are not sensitive to the privacy level of the model,
                    and consequently reveal the input.
                    On the other hand, attributions that show sensitivity to model privacy degrade sharply in visual coherence.
                    </div>
                </div>
                <div class="panel panel-default panel-paper">
                    <div class="panel-body panel-paper-body">
                        <span class="paper-author">
                        Amos Beimel, Aleksandra Korolova, Kobbi Nissim, Or Sheffet and Uri Stemmer
                        </span>
                        <br />
                        <a data-toggle="collapse" href="#abs42" class="paper-title">
                        The Power of Synergy in Differential Privacy: Combining a Small Curator with Local Randomizers
                        </a> &nbsp;&nbsp;
                        <a href="https://www.korolova.com/papers/power_of_synergy_in_differential_privacy.pdf" class="link-paper">[pdf]</a>
                    </div>
                    <div id="abs42" class="panel-footer panel-paper-footer collapse">
                    Motivated by the desire to bridge the utility gap between
                    local and trusted curator models of differential privacy for
                    practical applications, we initiate the theoretical study of
                    a hybrid model introduced by “Blender” [Avent et al., USENIX
                    Security ’17], in which differentially private protocols of
                    n agents that work in the local-model are assisted by a
                    differentially private curator that has access to the data
                    of m additional users. We focus on the regime where m &Lt; n
                    and study the new capabilities of this (m,n)-hybrid model.
                    We show that, despite the fact that the hybrid model adds no
                    significant new capabilities for the basic task of simple
                    hypothesis-testing, there are many other tasks (under a wide
                    range of parameters) that can be solved in the hybrid model
                    yet cannot be solved either by the curator or by the
                    local-users separately. Moreover, we exhibit additional
                    tasks where at least one round of interaction between the
                    curator and the local-users is necessary – namely, no hybrid
                    model protocol without such interaction can solve these tasks.
                    Taken together, our results show that the combination of the
                    local model with a small curator can become part of a
                    promising toolkit for designing and implementing
                    differential privacy.
                    </div>
                </div>
        </div>
        </div>
    </section>


    <!-- Call for travel grants -->
    <!--
        <section id="grants" class="container content-section text-center">
            <div class="row">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>Travel Grants</h2>
                    <p>
                    Thanks to our generous sponsors, we are able to provide a limited number of travel grants of up to $800 to help partially cover the expenses of authors of accepted papers who have not received other travel support from NeurIPS this year.
                    To apply, please send an email to <a href="mailto:ppml18@easychair.org?Subject=PPML18%20Travel%20Grant%20Application">ppml18@easychair.org</a> with the subject “PPML18 Travel Grant Application” including your resume and a half-page statement of purpose mentioning the title and the authors of your accepted paper and a summary of anticipated travel expenses. If you are an undergraduate or graduate student, we ask for a half-page recommendation letter supporting your application to be sent to us by the deadline. The deadline for applications is <b>November 11, 2018 (11:59pm AoE)</b>. The notifications will be sent by <b>November 16</b>. Please feel free to send us an email if you have any questions.
                </div>
            </div>
        </section>
    -->

    <!-- Organizers Section -->
    <section id="organizers" class="content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Organization</h2>
                <br />
                <h3>Workshop organizers</h3>
                <ul class="list-group">
                    <li class="list-group-item organizer">Borja Balle (DeepMind)</li>
                    <li class="list-group-item organizer">Kamalika Chaudhuri (UC San Diego)</li>
                    <li class="list-group-item organizer">Antti Honkela (University of Helsinki)</li>
                    <li class="list-group-item organizer">Antti Koskela (University of Helsinki)</li>
                    <li class="list-group-item organizer">Casey Meehan (UC San Diego)</li>
                    <li class="list-group-item organizer">Mijung Park (Max Planck Institute for Intelligent Systems)</li>
                    <li class="list-group-item organizer">Mary Anne Smart (UC San Diego)</li>
                    <li class="list-group-item organizer">Adrian Weller (Alan Turing Institute & Cambridge)</li>
                </ul>
                <br />
                <h3>Program Committee</h3>
                <ul class="list-group">
                  <li class="list-group-item organizer">James Bell (University of Cambridge)</li>
                  <li class="list-group-item organizer">Aurélien Bellet (INRIA)</li>
                  <li class="list-group-item organizer">Mark Bun (Boston University)</li>
                  <li class="list-group-item organizer">Christos Dimitrakakis (Chalmers University / University of Lille / Harvard University)</li>
                  <li class="list-group-item organizer">James Foulds (University of Maryland, Baltimore County)</li>
                  <li class="list-group-item organizer">Matt Fredrikson (Carnegie Mellon University)</li>
                  <li class="list-group-item organizer">Marco Gaboardi (University at Buffalo, SUNY)</li>
                  <li class="list-group-item organizer">Adria Gascon (The Alan Turing Institute / Warwick University)</li>
                  <li class="list-group-item organizer">Alon Gonen (Princeton University)</li>
                  <li class="list-group-item organizer">Peter Kairouz (Google AI)</li>
                  <li class="list-group-item organizer">Gautam Kamath (University of Waterloo)</li>
                  <li class="list-group-item organizer">Marcel Keller (Data61)</li>
                  <li class="list-group-item organizer">Nadin Kokciyan (King's College London)</li>
                  <li class="list-group-item organizer">Aleksandra Korolova (University of Southern California)</li>
                  <li class="list-group-item organizer">Audra McMillan (Boston University and Northeastern University)</li>
                  <li class="list-group-item organizer">Olga Ohrimenko (Microsoft)</li>
                  <li class="list-group-item organizer">Jun Sakuma (University of Tsukuba)</li>
                  <li class="list-group-item organizer">Anand Sarwate (Rutgers University)</li>
                  <li class="list-group-item organizer">Phillipp Schoppmann (Humboldt University of Berlin)</li>
                  <li class="list-group-item organizer">Or Sheffet (University of Alberta)</li>
                  <li class="list-group-item organizer">Kana Shimizu (Computational Biology Research Center, AIST)</li>
                  <li class="list-group-item organizer">Thomas Steinke (IBM)</li>
                  <li class="list-group-item organizer">Kunal Talwar (Google)</li>
                  <li class="list-group-item organizer">Carmela Troncoso (Ecole Polytechnique Fédérale de Lausanne)</li>
                  <li class="list-group-item organizer">Yu-Xiang Wang (Carnegie Mellon University)</li>
                  <li class="list-group-item organizer"></li>
                </ul>
                <!-- <br />
                <h3>Sponsors</h3>
                <br />
                    <img style="margin:50px;"height="80" src="img/ati-white.png"> -->
                    <!-- <img style="margin:50px;"height="80" src="img/amazon.png">
                    <img style="margin:50px;"height="80" src="img/google.png">
                    <img style="margin:50px;"height="80" src="img/microsoft.png"> -->
            </div>
        </div>
    </section>

    <!-- Accessibility Section -->
    <section id="access" class="content-section text-center">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2">
                <h2>Accessibility</h2>
                <br />
                <p>
                  By taking a few simple steps&mdash;such as paying special attention to font sizes and captions&mdash;
                  you can make your
                  <a href="https://www.washington.edu/doit/how-can-you-make-your-presentation-accessible">presentations</a> and
                  <a href="resources/accessibility_posters_gilson2019.pdf">posters</a>
                  more accessible.
                  Feel free to <a href="mailto:priml2019@easychair.org">contact us</a> about any accessibility concerns relating to the website, workshop, etc.
                </p>
                <br />
                <br />
                <br />
                <br />
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container text-center" >
          <p>Sponsored by</p>
            <a href="https://www.turing.ac.uk/"><img height="50" style="margin:30px;" src="img/ati-white.png"></a>
            <a class="hide-on-mobile" href="http://lcfi.ac.uk/"><img height="50" style="margin:20px;" src="img/cfi.jpg"></a>
            <a class="mobile-only" href="http://lcfi.ac.uk/"><img width="275" style="margin:10px;" src="img/cfi.jpg"></a>
            <a href="https://deepmind.com/"><img height="50" style="margin:30px;" src="img/DM.png"></a>
        </div>
        <div class="container text-center">
            <p>Contact us: <a href="mailto:priml2019@easychair.org">priml2019@easychair.org</a></p>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="js/jquery.min.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>

    <!-- Theme JavaScript -->
    <script src="js/script.js"></script>

</body>

</html>
